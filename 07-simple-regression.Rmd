```{r include=FALSE}
library(dplyr)
```

# (PART) Statistical Analysis {-} 

# Simple Regression
> In this chapter we'll use the `lm()` function for fitting a simple regression model with one continuous predictor, which will take the following forms: 

$$y_i=\beta_0+\beta_1\left(X1\right)+\epsilon_i$$
$$y_i=\beta_0+\beta_1\left(X1-\overline{X1}\right)+\epsilon_i$$

> The top equation is an untransformed model, and the bottom is a mean-centered model.

> The code in this chapter only works if you're following along with the Github folder for this book (which you can download [here](https://github.com/JasonDude16/Statistical-Analysis-Folder)), you've correctly set your working directory to the data folder (which you can learn how to do in [Chapter 4](#CH4)), and run the code in the order it appears in this chapter.

> The data used in this chapter is from Kaggle: <https://www.kaggle.com/open-powerlifting/powerlifting-database>. This dataset is a snapshot of the OpenPowerlifting database as of April 2019.

## Importing {-}
The first step is importing the data. In this example we'll use the **powerlifting.csv** file in the data folder, which is a dataset that consists of competitors and their statistics at a Powerlifting competition:

```{r}
data <- read.csv("powerlifting.csv")
```

## Viewing {-}
You can use the `str()` and `head()` functions to get the overall "impression" of the dataset.

```{r}
str(data)
```

The `str()` function provides a lot of good information. We now know that the dataset was correctly imported as a data frame which consists of 1000 observations of 27 variables, and we know the column names and column variable types. Additionally, the first few observations for each column is printed. But if you'd rather look at the dataset in a more standard format, you can use the `head()` function to see the first few observations:  

```{r}
head(data[1:9])
# Only the first 9 columns are printed here to save space
# By default, the first 6 observations of all columns are printed
```

Let's hypothesize that competitors who weigh more will have a higher `TotalKg`, which is the sum of the competitor's most weight lifted on three lifts: Squat, Bench Press, and Deadlift. First, let's visualize the relationship between these variables.

```{r out.height= "70%", out.width= "70%"}
plot(data$BodyweightKg, data$TotalKg)
```

> It looks like data points are concentrated more heavily at specific body weights. This is likely because powerlifters compete in weight classes, and its advantageous to be as close as possible to the cutoff weight limit. 

We can use the `hist()` function to visualize the distributions of each variable:

```{r out.height= "70%", out.width= "70%"}
par(mfrow = c(1, 2))
hist(data$TotalKg)
hist(data$BodyweightKg)
```

> The code `par(mfrow = c(1,2))` above is used to print the plots as a 1 x 2 grid; the `plot()` function does not need to be used in conjuction with this piece of code.

And you can look at the six number summary with the `summary()` function. Notice how the summary conveniently includes the `NA` count.

```{r}
summary(data[c("TotalKg", "BodyweightKg")])
```

> More examples of viewing data can be found in [Chapter 5](#CH5)

## Formatting {-}
Before modeling, we need to ask ourselves a few questions. What if someone didn't perform one of the lifts? Powerlifting competitions have several event options, and we might be including in our analysis individuals who have a `TotalKg` value, but their value is only for the squat and bench press lifts, rather than all three. Also, what if a competitor didn't successfully complete a lift? That is, they competed in all three lifts, but they (likely) attempted to lift too much weight and their attempts were unsuccessful so they were disqualified. It probably isn't a good idea to include either of these scenarios in our analysis because their `TotalKg` values wouldn't represent the summation of all three lifts, which is what we're interested in. We could use the `filter()` function from the `tidyverse` package to filter the data and take care of both of these issues in one step. Make sure the `tidyverse` package is loaded before running this code:

```{r}
# Replace the original data object with the filtered data
data <- filter(data,  Event == "SBD", Place != "DQ")
```

In the code block above, the data was filtered to include only competitors who competed in all three lifts, `SBD`, and they were not disqualified, `DQ`, meaning that they had at least one successful lift on all three lifts. This ensures that the `TotalKg` value is representing the same value for all competitors.

> More examples of formatting data can be found in [Chapter 6](#CH6)

## Modeling {-}

### The `lm()` Function {-}

```{r, eval=FALSE}
lm(formula, data, subset, weights, na.action,
   method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
   singular.ok = TRUE, contrasts = NULL, offset, ...)
```

The `lm()` (linear model) function is used for fitting linear models. There are many arguments for this function, but the `formula` and `data`arguments are the only ones that *need* to be specified. If you'd like to learn more about functions and arguments, [Chapter 2](#CH2) covers basic programming concepts, including functions and arguments.

We'll use the `lm()` function to make two models: the untransformed linear model and a mean-centered model.

### Untransformed Model {-} 
When using the `lm()` function, the `formula` argument is set equal to the dependent variable, followed by a tilde, `~`, and then the independent variable(s). The `data` argument is set equal to the object that contains the dataset, which in this example is the object called `data`.

```{r}
lm(formula = TotalKg ~ BodyweightKg, data = data)
```

The function prints out the slope and intercept for the model. We could then use the slope and intercept to create a plot with an `abline`:

```{r fig.align='center', out.height= "70%", out.width= "70%"}
plot(data$BodyweightKg, data$TotalKg)
abline(50.83, 5.24, col = "red")
```

It looks like there may be a significant relationship between the two variables, but how can we be sure? The `lm()` function printed the coefficients but did not provide information about the R-squared or significance. To see this information, we need to save the model as an object, and then print the summary of the model:

```{r}
my_model <- lm(formula = TotalKg ~ BodyweightKg, data = data)
summary(my_model)
```

Now we have a nice summary print-out which includes the `F-statistic`, `Residuals`, `R-squared`, `p-value`, and more. What's also nice is we can now use the `plot` function on the `my_model` object that we just created to view `Residuals vs Fitted`, `Normal Q-Q`, `Scale-Location` and `Residuals vs Leverage` plots.

```{r fig.align='center', out.height= "70%", out.width= "70%"}
par(mfrow = c(2,2))
plot(my_model)
```

> The code `par(mfrow = c(2,2))` above is used to print the plots as a 2 x 2 grid; the `plot()` function does not need to be used in conjuction with this piece of code.

What else can be done with the `my_model` object? Let's take a look at the object's `attributes`:

```{r}
attributes(my_model)
```

The model's `attributes` can be accessed by using a dollar sign, `$`. For example, here's a printout of the first 5 residuals of the model:

```{r}
my_model$residuals[1:5]
```

### Mean-Centered Model {-}
In this example we'll use the same variables as before, but this time the predictor variable will be mean-centered. First, we create a column that consists of the `BodyweightKg` mean, which is `r {round(mean(data$BodyweightKg), digits=2)}`. Since there are 1000 rows in the powerlifting dataset, that means we are creating a column that has the value `r {round(mean(data$BodyweightKg), digits=2)}` repeated 1000 times. This value is then saved into the column `BodyweightKg_mean`; that's what the first line of code in the code chunk below is doing. In the second line of code, each `BodyweightKg` value is subtracted from the `BodyweightKg_mean` mean column, which is then stored in a new column called `BodyweightKg_mc`. 

```{r}
# Create a column that consists of the mean BodyweightKg
data$BodyweightKg_mean <- mean(data$BodyweightKg, na.rm = TRUE) 

# Subtract BodyweightKg from mean BodyweightKg column
data$BodyweightKg_mc <- data$BodyweightKg - data$BodyweightKg_mean
```

Here's what the new mean-centered column looks like graphically:

```{r fig.align='center', out.height= "70%", out.width= "70%"}
plot(data$BodyweightKg_mc ,data$TotalKg)
abline(lm(data$TotalKg ~ data$BodyweightKg_mc), col = "red")
```

We can now use this mean-centered column as the dependent variable in the `lm()` function.

```{r}
my_model2 <- lm(formula = TotalKg ~ BodyweightKg_mc, data = data)
summary(my_model2)
```

The slopes are the same in both models, but in the mean-centered model the y-intercept is now the average `TotalKg`.