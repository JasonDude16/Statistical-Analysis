```{r include=FALSE}
library(dplyr)
```

# (PART) Statistical Analysis {-} 

# Simple Regression
> In this chapter we'll use the `lm()` function for fitting a simple regression model with one continuous predictor, which will take the following forms: 

$$y_i\ =\ \beta_0\ +\ \beta_1\left(x_i\right)\ +\ e_i$$
$$y_i=\beta_0+\beta_1\left(x_i\ -\ \overline{x}\right)\ +\ e_i$$

> The top equation is an untransformed model, and the bottom is a mean-centered model.

> The code in this chapter only works if you're following along with the Github folder for this book (which you can download [here](https://github.com/JasonDude16/Statistical-Analysis-Folder)), you've correctly set your working directory to the data folder (which you can learn how to do in [Chapter 4](#CH4)), and run the code in the order it appears in this chapter.

> The data used in this chapter is from Kaggle: <https://www.kaggle.com/open-powerlifting/powerlifting-database>. This dataset is a snapshot of the OpenPowerlifting database as of April 2019. Powerlifting is a sport in which competitors compete to lift the most weight for their class in three separate barbell lifts: the Squat, Bench, and Deadlift.

## Importing {-}
The first step is importing the data. In this example we'll use the **powerlifting.csv** file in the data folder, which is a dataset that consists of competitors and their statistics at a Powerlifting competition:

```{r}
data <- read.csv("powerlifting.csv")
```

## Viewing {-}
You can use the `str()` and `head()` functions to get the overall "impression" of the dataset.

```{r}
str(data)
```

The `str()` function provides a lot of good information. We now know that the dataset was correctly imported as a data frame which consists of 1000 observations of 27 variables, and we know the column names and column variable types. Additionally, the first few observations for each column is printed. But if you'd rather look at the dataset in a more standard format, you can use the `head()` function to see the first few observations:  

```{r}
head(data[1:9])
# Only the first 9 columns are printed here to save space
```

Let's hypothesize that competitors who weigh more will have a higher `TotalKg`, which is the sum of the competitor's most weight lifted on three lifts: Squat, Bench Press, and Deadlift. First, let's visualize the relationship between these variables.

```{r out.height= "70%", out.width= "70%"}
plot(data$BodyweightKg, data$TotalKg)
```

> It looks like data points are concentrated more heavily at specific body weights. This is likely because powerlifters compete in weight classes, and its advantageous to be as close as possible to the cutoff weight limit. 

We can use the `hist()` function to visualize the distributions of each variable:

```{r out.height= "70%", out.width= "70%"}
par(mfrow = c(1, 2))
hist(data$TotalKg)
hist(data$BodyweightKg)
```

And you can look at the six number summary with the `summary()` function. Notice how the summary conveniently includes the `NA` count.

```{r}
summary(data[c(9,26)])
```

> More examples of viewing data can be found in [Chapter 5](#CH5)

## Formatting {-}
Before modeling, we need to ask ourselves a few questions. What if someone didn't perform one of the lifts? Powerlifting competitions have several event options, and we might be including in our analysis individuals who have a `TotalKg` value, but their value is only for the squat and bench press lifts, rather than all three. Also, what if a competitor didn't successfully complete a lift? That is, they competed in all three lifts, but they (likely) attempted to lift too much weight and their attempts were unsuccessful. It probably isn't a good idea to include either of these scenarios in our analysis because their `TotalKg` values wouldn't represent the summation of all three lifts, which is what we're interested in. We could use the `filter()` function from the `dplyr` package to filter the data and take care of both of these issues in one step:

```{r}
# Replace the original data object with the filtered data
data <- filter(data,
               !is.na(Best3SquatKg), 
               !is.na(Best3BenchKg),
               !is.na(Best3DeadliftKg),
               !is.na(TotalKg))
```

In the code block above, the data was filtered to include only observations that had a successful squat, bench press, and deadlift, and had a `TotalKg` value. This ensures that the competitor competed in all three lifts and also had at least one successful attempt on each lift. The data is now ready for modeling. 

> More examples of formatting data can be found in [Chapter 6](#CH6)

## Modeling {-}

### The `lm()` Function {-}

```{r, eval=FALSE}
lm(formula, data, subset, weights, na.action,
   method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
   singular.ok = TRUE, contrasts = NULL, offset, ...)
```

The `lm()` (linear model) function is used for fitting linear models. There are many arguments for this function, but the `formula` and `data`arguments are the only ones that *need* to be specified. If you'd like to learn more about functions and arguments, [Chapter 2](#CH2) covers basic programming concepts, including functions and arguments.

Let's take a look at how we can use the `lm()` function to make two models with the same variables using a simple regression: the untransformed linear model and a mean-centered model. For both models, `TotalKg` will be the dependent variable and `BodyWeightKg` will be the independent variable. 

### Untransformed Model {-} 
The equation above represents the untransformed, simple regression model with one continuous predictor. For the `formula` argument, the dependent variable is listed first, followed by a tilda, `~`, and then the independent variable(s). For the `data` argument, you'll use the `data` object.

```{r}
lm(formula = TotalKg ~ BodyweightKg, data = data)
```

The function prints out the slope and intercept for the model. We could then use the slope and intercept to create a plot with an `abline`:

```{r fig.align='center', out.height= "70%", out.width= "70%"}
plot(data$BodyweightKg, data$TotalKg)
abline(50.83, 5.24, col = "red")
```

It looks like there may be a significant relationship between the two variables, but how can we be sure? The `lm()` function printed the coefficients but did not provide information about the R-squared or significance. To see this information, we need to save the model as an object, and then print the summary of the model, like so:

```{r}
my_model <- lm(formula = TotalKg ~ BodyweightKg, data = data)
summary(my_model)
```

Now we have a nice summary print-out which includes the `F-statistic`, `Residuals`, `R-squared`, `p-value`, and more. What's also nice is we can now use the `plot` function on the `my_model` object that we just created to view `Residuals vs Fitted`, `Normal Q-Q`, `Scale-Location` and `Residuals vs Leverage` plots.

```{r fig.align='center', out.height= "70%", out.width= "70%"}
par(mfrow = c(2,2))
plot(my_model)
```

What else can we do with the `my_model` object? Let's take a look at the object's `attributes`:

```{r}
attributes(my_model)
```

The model's `attributes` can be accessed by using a dollar sign, `$`. For example, here's a printout of the first 5 residuals of the model:

```{r}
my_model$residuals[1:5]
```

### Mean-Centered Model {-}
In this example we'll use the same variables as before, but this time the predictor variable will be mean-centered. First, we create a column that consists of the `BodyweightKg` mean, which is `r {round(mean(data$BodyweightKg), digits=2)}`. Since there are 1000 rows in the powerlifting dataset, that means we are creating a column that has the value `r {round(mean(data$BodyweightKg), digits=2)}` repeated 1000 times. This value is then saved into the column `BodyweightKg_mean`; that's what the first line of code in the code chunk below is doing. In the second line of code, each `BodyweightKg` value is subtracted from the `BodyweightKg_mean` mean column, which is then stored in a new column called `BodyweightKg_mc`. 

```{r}
# Create a column that consists of the mean BodyweightKg
data$BodyweightKg_mean <- mean(data$BodyweightKg, na.rm = TRUE) 

# Subtract BodyweightKg from mean BodyweightKg column
data$BodyweightKg_mc <- data$BodyweightKg - data$BodyweightKg_mean
```

Let's see what it looks like:

```{r fig.align='center', out.height= "70%", out.width= "70%"}
plot(data$BodyweightKg_mc ,data$TotalKg)
abline(lm(data$TotalKg ~ data$BodyweightKg_mc), col = "red")
```

We can now use this mean-centered column as the dependent variable.

```{r}
my_model2 <- lm(formula = TotalKg ~ BodyweightKg_mc, data = data)
summary(my_model2)
```

Notice how the slopes are the same in both models, but in the mean-centered model the y-intercept is now the average `TotalKg`.