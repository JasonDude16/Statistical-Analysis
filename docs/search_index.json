[
["index.html", "Welcome", " Welcome The purpose of this guidebook is to provide a brief introduction to performing statistical analysis in R, and it is intended for a reader who 1) Is knowledgeable about statistical analysis and is selecting the appropriate statistical model for whatever research question they would like to answer, and 2) Is unsure of how to perform the analysis in R, and is unfamiliar with programming languages in general. We tried to make this book as brief as possible while still providing all of the necessary information to get up and running with R. The book is sequential and builds on knowledge from prior chapters, and, as such, we recommended reading it in order. Chapters 1 and 2 will cover downloading and describing R and the RStudio IDE, as well as programming basics, such as variable types and data types, objects and assignments, and functions and arguments. Chapters 3 and 4 will cover installing/loading packages and reading/writing files. Chapters 5 and 6 will cover descriptive statistics, visualizing data, and formatting data. Chapters 7-13 will cover various statistical models from the General Linear Model framework. "],
["resources.html", "Resources", " Resources Link Description Swirl Swirl is an R package where you can learn and practice R, in R. Click on the link provided to learn more about installing and using swirl. StackOverflow Need help with R? Stackoverflow is a website for posting programming related questions. Quick R by Datacamp Quick R by Datacamp is a webpage where the content is very simmilar to that of this guidebook: short and to the point, but still covering everything from installing R to statistics. R Programming for Data Science R Programming for Data Science is a comprehensive introduction to R and data analysis written by Roger D. Peng, a professor of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. "],
["getting-started-with-r.html", "1 Getting Started with R What is R? Installing R What is RStudio? Installing RStudio Opening R or RStudio The RStudio Layout Following Along Opening a Script Saving a Script", " 1 Getting Started with R This chapter will walk through the steps of getting started with R and RStudio. At the end of the chapter, we will provide a link where you can download a folder that includes all of the code and data that is used in this guidebook. What is R? R is an open-source, free software environment for data manipulation and statistical computing. Here is an excerpt from the R website (https://www.r-project.org): The term “environment” is intended to characterize R as a fully planned and coherent system, rather than an incremental accretion of very specific and inflexible tools, as is frequently the case with other data analysis software. A programming language offers more flexibility than other data analysis tools like Excel and SPSS, but the benefit of flexibility comes with the cost of complexity. The learning curve of R is steeper than Excel and SPSS, but once you get the hang of it, R can streamline and simplify many tasks. Have you ever needed to copy and paste many data files together into one “master” file? This method is both error-prone and time-consuming. A task like this can be accomplished in a few lines of code in R, and the code can then used as a template for performing the same kind of task in the future. This is just one example, but R can be used for many other time-consuming tasks like this. The sky is the limit! Installing R Step 1 Go to the R website (https://www.r-project.org/) and click on the download R link that’s circled below. Step 2 You’ll then be redirected to a page that looks like this. If you you scroll down you’ll see several USA locations listed. Click on one of those links (it doesn’t matter which one you choose. For example, I chose the St. Louis location). Step 3 Now click on the download link that is relevant to your operating system. Step 4: Mac users If you’re a mac user, you’ll see this screen next. The version of R you download will depend on which version of software your mac is running. At the time of writing this, the latest R release is R 4.0.0 which is for mac 10.13 OS (High Sierra) and higher. If you have an older version of mac software, you can download an older version of R from the same page. Step 4: Windows users If you’re a windows user you’ll see this screen next, where you’ll want to click on the “install R for the first time” link. You can then download R by clicking on this link. The exact version will likely be different than the one shown here. What is RStudio? Simply put, RStudio IDE is an environment that makes working with R much less painful. “IDE” stands for integrated development environment. Without RStudio, R looks something like this: As you can see it’s a bit disorganized. The RStudio IDE cleans up the R interface so that it’s much cleaner and easier to work with, like this: You don’t need to use RStudio, but we highly recommend that you do because it significantly improves the user experience, especially for new users. Installing RStudio Go to the RStudio website (https://rstudio.com/) and click the download button. Then scroll down and click on the download link that is relevant to your operating system. Opening R or RStudio If you decided not to download RStudio, then you’ll work with R by opening the R application. It’s perfectly fine to do this, but just know that you’ll be on your own! The rest of this book will assume you’ve installed RStudio, and we’ll occasionally make references that are specific to the RStudio interface. If you downloaded RStudio, you’ll work with R by opening RStudio. In this case, you should not open R, because RStudio is simply an interface for R; by using RStudio you are simultaneously using R. So, if you were to open both R and RStudio, you would be starting two separate R sessions, which is not what you want to do. Hopefully that makes sense, but to be abundantly clear: RStudio is the only application you need to open when using R. Now would be a good time to open RStudio and start following along. The RStudio Layout By default, RStudio uses the 4 pane layout shown below. Pane A: This is where you write your scripts. You can think of a script like a notepad, where you can write your code and then save it for later. Pane B: This is where your code is evaluated, and specifically it is evaluated under the Console tab (you can ignore the Terminal tab). What does it mean to ‘evaluate’ code? Let’s say you wanted to calculate the mean of 3 numbers, which could be done like this in R: mean(c(12, 32, 15)). We then evaluate the code to get the answer. The result, 19.7, would be displayed in Pane B under the Console tab. You can also write code here. For example, you could write mean(c(12, 32, 15)) directly into the console (instead of in Pane A), but keep in mind that any code that is typed here will not be saved. Pane C: This pane has several tabs, but the most important for our purposes is the Enironment tab (if you don’t see a Build or Git tab, that’s okay). The Environment tab is where you can view your data and the objects you create, which will make more sense as you progress through the book. Pane D: In this pane you can view files, plots, install packages, get help with R functions, and more. The help tab is very useful when learning how to use R; this is where you can learn about any function you may want to use by viewing the provided documentation (which is very good in R!). Following Along We wanted to make it as easy as possible to follow along with this guidebook, so we created a folder that contains all of the data and code that’s used throughout the chapters, which can be downloaded here. After clicking on the link, you should be redirected to a page that has a green “Clone or download” button, shown below. Note: since writing this guidebook, the GitHub website has changed slightly. At the time of this update, the green button says “Code”. Aside from this, the instructions are still the same . You can then click on the dropdown arrow on the green button and click Download ZIP. Next, find the folder on your computer, unzip it, and move it to a convenient location (such as your Desktop). If you currently have RStudio open, go ahead and close it. After you’ve closed RStudio, open the file within the folder that ends with the extension .Rproj. After you click on the Rproj file, RStudio should open and you should see a screen similar to the one below. By opening the .Rproj file, we opened an R project. There’s nothing particularly “special” about an R project; all it really does is set the working directory to the correct location, which will make more sense after reading Chapter 4. Doing this will ensure the code in the following chapters will work as expected. Opening a Script You can open a script in R by clicking on the dropdown arrow next to the white piece of paper with a green plus sign that’s located in the upper left corner. Then click on R Script. You should now see a new pane added to the upper left section of your screen, which is the R script. Let’s add a comment to the script so it’s no longer blank. Try copying and pasting the following lines into the script: # This is my first R script 10 * 10 # This is my first R script is simply a comment, which is denoted by the hashtag. 10 * 10, which is code that will multiply the numbers 10 and 10, will be evaluated once the code has been ran. You can run the code by pressing Ctrl + Enter on a Windows or Cmd + Enter on a Mac. Doing this will run only the selected line of code, meaning that your cursor must be on the line you want to evaluate in order for it to work. To run the code 10 * 10, place your cursor before or after the expression (or you can highlight the expression) and press Ctrl/Cmd + Enter. You should see [1] 100 printed to the console (lower left pane). Saving a Script You can save a script by clicking on File &gt; Save as, and navigating to the folder you’d like to save it in. It doesn’t matter too much where you choose to save a script, but it’s good practice to create a scripts folder and save it there. Let’s say you’re performing an analysis related to VO2 max. A good folder setup would be creating a main folder called VO2max_Analysis (or something like that) and then create a scripts folder and data folder within that folder, which will contain your R scripts and data, respectively. "],
["CH2.html", "2 Programming 101 Running R Code R is a calculator Objects and Assignment Variable Types Data Types Dollar Signs and Brackets Functions and Arguments Code Formatting", " 2 Programming 101 The goal of this chapter is not to turn you into an R programmer, but to cover the most important programming concepts so that you feel more comfortable using R and are able to get up and running with your analyses as fast as possible. Running R Code The rest of this booklet will contain code blocks that look like this: # This is a code block, and this is an example of a comment. # You can comment your script by using the hashtag. # That way you can make notes to yourself and keep track of what your code # is doing at each step. You can copy and paste the code within the blocks into your R script to try it out for yourself; but the code above is just a comment so it won’t do anything exciting. The code blocks will often (but not always) have some sort of output printed directly below them so that you can see the results. For example: mean(1:10) [1] 5.5 In the example above, the code block consisted of code that calculated the mean of the numbers 1 through 10, and the results were printed directly below. Notice how the output is printed in black font, which is how you can distinguish the code blocks from the output blocks. You can “run” a line of code in R by pressing Cmd + Enter on a Mac and Ctrl + Enter on a Windows. For example, if you wanted to know what 75 times 82 is, you could type 75 * 82 into your script and “run” the code by using Cmd + Enter or Ctrl + Enter to get the answer. Doing this will run only the selected line of code, meaning that your cursor must be on the line you want to evaluate in order for it to work. You can also type code directly into the console and run it by hitting Enter. “Why would I type code directly into the console?” You might want to type code directly into the console when you’re performing calculations but you don’t care about saving the results. A script is like a notepad where you can save your analysis. If you needed to calculate 75 * 82, it’s probably not essential to have that calculation saved to your notepad, so you can type it directly into the console to keep your notepad clean. That way, you won’t have to constantly delete irrelevant code and calculations from your script. R is a calculator Try out R’s calculator functionality by copying and pasting the code below into your R session. 21 + 21 # Addition 43 - 20 # Subtraction 4 * 4 # Multiplication 45 / 3 # Division 2 ^ 3 # Exponentiation sqrt(16) # Square root # R follows PEMDAS rules (((3 + 2)^2) + (10 * 2)) - 4 (3 + 2)^2 + 10 * 2 - 4 Objects and Assignment One of the most basic programming concepts is assignment. Let’s say we want the letter x to refer to the value 5. You can do this by assigning x to the value 5 with the assignment operator, &lt;-, or with the equal sign operator, =: x &lt;- 5 x = 5 In both of these examples the result is the same: x is now assigned to the value 5. It’s a matter of personal preference which operator you’d like to use (&lt;- or =), but it’s best to be consistent to make your script more readable. x is now considered an object. An object is a name used to reference a value or set of values. The object x refers to the value 5. Instead of just assigning the number 5 to x, we could perform several calculations and then assign the result to an object, like so: x &lt;- sqrt(34 + ((12 * 2)^3 - 5)) x # Typing x by itself, like this, will print the value to the console [1] 117.6988 The object x no longer refers to 5. We overwrote x and assigned it to a new value, ~117.7. When analyzing data, you’ll likely need to transform the data before you can run statistical tests. You can accomplish this by transforming the data and then saving the data as a new object. Let’s look at one more example. head(iris) # The head() function prints the first few observations from the dataset Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa Here’s one of R’s built-in datasets, which is called iris and consists of 3 flower species (setosa, virginica, versicolor) and their their petal characteristics. Let’s say we only wanted to include virginica flowers in our analysis. subset(iris, Species == &quot;virginica&quot;) The code above filters the data to include only virginica plants, but we didn’t assign it to an object so we have no way of referencing our filtered data. iris_virginica &lt;- subset(iris, Species == &quot;virginica&quot;) head(iris_virginica) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 101 6.3 3.3 6.0 2.5 virginica 102 5.8 2.7 5.1 1.9 virginica 103 7.1 3.0 5.9 2.1 virginica 104 6.3 2.9 5.6 1.8 virginica 105 6.5 3.0 5.8 2.2 virginica 106 7.6 3.0 6.6 2.1 virginica The code above looks much better. The filtered data has been assigned to the object iris_virginica, so we now have a way to reference that data, and we could use this object to analyze the virginica plants exclusively. Variable Types R has several different variable types, and to manipulate your data it is good to be aware of them. 0.5, 2 # Numeric class TRUE, FALSE # Logical class &quot;hat&quot;, &quot;dog&quot; # Character class 5L, 12L # Integer class (&quot;L&quot; stores the value as an integer) 1+0i, 2+4i # Complex class &quot;yes&quot;, &quot;no&quot; # Factor class (categorical variable; each level is a category) These variable types are the basic building blocks, and all values in R belong to one of them. You may have a situation where your data is imported as character but you’d like to change it to factor, or vice versa. But first, what exactly is a character class and what is a factor class? In the code block above, the example shows that \"hat\" and \"dog\" are of the character class; this means that R will interpret these values as letters and words, and nothing more. A factor class, on the other hand, will have an extra layer of complexity. Factors will be interpreted as categories, and not simply words. In the code block above the example given is yes and no as being of the factor class. Let’s say you had a dataset with 100 observations and 50 observations responded to a question with “yes” and 50 with “no”. If this were a character class, you would not be able to treat “yes” and “no” as categories. In order to be treated as categories, you would need to convert the values to a factor class that has two levels: yes and no. You can easily do this with the as.factor() and as.character() functions, which will be discussed in greater detail in Chapter 6. y &lt;- c(1,2,3,4,5) y [1] 1 2 3 4 5 When you combine values, it’s called concatenation. In the above example, numeric values were concatenated together and assigned to the object y The object y now refers to 5 values: 1, 2, 3, 4, 5. When we concatenate values together like this, we create a data type. Data Types R has many data types, but the only one we’ll worry about is the data frame. A data frame is usually the object that’s created when importing a data file into R. A data frame consists of columns and rows, where each column is the same length. Here’s an example of one of R’s built-in data frames called mtcars: head(mtcars) mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Each column within the data frame consists of only one variable type, and this is always the case for data frames. In the mtcars data frame above, all columns are numeric. Even though some may look like integers or factors, they are not. How variable types can be changed will be discussed in Chapter 6. Dollar Signs and Brackets When working with datasets, you’ll often want to extract/reference part of the dataset. For example, in the mtcars dataset above, you might want to visualize the relationship between the hp (horsepower) and mpg of cars. We need to have a way of referencing these specific columns within the dataset; this can be accomplished with brackets, [], and dollar signs, $. Brackets and dollar signs allow you to subset the data. For example, here’s a plot of the hp and mpg relationship: plot(mtcars$hp, mtcars$mpg) In the code above, the dollar sign was used to selected the hp and mpg columns from the mpg data frame. Although not the most precise definition, you can think of the dollar sign as allowing you to select part of an object. In the example above, the column was the “part” that was selected, but other “parts” can be selected as well, which you’ll see in the statistical modeling section. Similarly to the dollar sign, brackets can be used to subset: head(mtcars[1]) mpg Mazda RX4 21.0 Mazda RX4 Wag 21.0 Datsun 710 22.8 Hornet 4 Drive 21.4 Hornet Sportabout 18.7 Valiant 18.1 In the code above, the first column of the mtcars dataset was selected, which is the mpg column (the head() function was also used so that only the first few observations were printed). You can use brackets to subset both the columns and the rows of a dataset: mtcars[1:6, 1] [1] 21.0 21.0 22.8 21.4 18.7 18.1 In the code above, the first 6 rows of the first column were selected from the mtcars dataset. Functions and Arguments So far we’ve seen R as a calculator, created objects, learned about variable types and data types, and subset data. But how do we actually do things in R? That is, how do we manipulate data and run a statistical analysis? To do just about anything in R you use functions. What functions do in R is take a process that would normally be a hassle for the user and make it easier. For example, the mean() function. We’re all familiar with how the mean is calculated, and we could calculate this manually in R. If we wanted the mean of the numbers c(1, 3, 4, 4) we could simply sum the numbers and divide by n to get 3. But why do that when we can just use the mean() function? And that’s the basic idea of a function: to simplify things for us. The lm() function creates a linear model from our data, which again we could achieve the same result by manually performing many calculations, but this would be a time-consuming, arduous task, and using the lm() function greatly simplifies the process. Functions are great because they can simplify and streamline a process, but how do you make the function do exactly what you need to do in your analysis? Let’s take a look at the factor() function to get an idea of how to take advantage of functions and arguments. Factors are categorical variables, where each level is a category. Here is an example of character data that’s been converted to a factor with the factor() function: y &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;dog&quot;, &quot;cat&quot;, &quot;dog&quot;) y &lt;- factor(y) # y is converted to a factor with 2 levels y [1] cat dog cat cat dog cat dog Levels: cat dog In this example, two levels are created from the data: cat and dog. Let’s slightly modify this example and say that data was entered incorrectly into a column, so the column looks like this: y &lt;- c(&quot;cat&quot;, &quot;dg&quot;, &quot;cat&quot;, &quot;cart&quot;, &quot;dog&quot;, &quot;cat&quot;, &quot;dog&quot;) y &lt;- factor(y) y [1] cat dg cat cart dog cat dog Levels: cart cat dg dog Because of the typos, we now have 4 levels–cat, dg, cart, dog– and that’s clearly not what we want. To make the most of the factor() function, we can make use of its arguments, which allows us to have greater control over what the function does and how it manipulates the data. We could ignore the typos by using the levels argument, which is a way of specifying how many levels we’re supposed to have for the data: y &lt;- c(&quot;cat&quot;, &quot;dg&quot;, &quot;cat&quot;, &quot;cart&quot;, &quot;dog&quot;, &quot;cat&quot;, &quot;dog&quot;) y &lt;- factor(y, levels = c(&quot;cat&quot;, &quot;dog&quot;)) y [1] cat &lt;NA&gt; cat &lt;NA&gt; dog cat dog Levels: cat dog We’re now back to two levels, and the typos were treated as NA. Now let’s say we wanted to recode cat as 0 and dog as 1. We could use the labels argument, which creates a new label for the corresponding level (in order): y &lt;- c(&quot;cat&quot;, &quot;dg&quot;, &quot;cat&quot;, &quot;cart&quot;, &quot;dog&quot;, &quot;cat&quot;, &quot;dog&quot;) y &lt;- factor(y, levels = c(&quot;cat&quot;, &quot;dog&quot;), labels = c(0, 1)) y [1] 0 &lt;NA&gt; 0 &lt;NA&gt; 1 0 1 Levels: 0 1 By using factor()’s arguments we were able to get the function to do what we wanted it to do, not just what it does by default. This is just one example of what makes a programming language like R so powerful! Example Now let’s look at a more complete example of functions and arguments with the lm() function. lm(formula, data, subset, weights, na.action, method = &quot;qr&quot;, model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, offset, ...) Above is the lm() function used for fitting linear models. The function has many arguments, but most of the arguments have default values; what this means is that you do not need to assign a value to that argument in order for the function to work. For example, in this function the default method argument is set to qr: method = \"qr\". This means that when you use the lm() function, the method will be set to qr even if you don’t explicitly set it to qr; hence, qr is the default (which is referring to QR Decomposition, in case you were curious). The idea of a default argument is very important point, because the default arguments for a function can impact the way the function behaves without the user being aware of it. Keep this in mind if you’re using a function and the results are not what you’d expect them to be. In addition to arguments that have default values, there are also arguments which are optional. In the lm() function above, subset, weights, na.action, and offset are all optional arguments. Unsurprisingly, this means that these arguments do not need to be used, but they can be. We can now see why the code below is valid, even though there are many arguments that were not specified. lm(formula = mpg ~ hp, data = mtcars) We can also see why this code below is valid. The subset argument is optional, and here we used it to specify that we only want to include the first 10 observations in our model. lm(formula = mpg ~ hp, data = mtcars, subset = c(1:10)) “But how do I know what the arguments are for a given function, what they do, which arguments have default values, which ones are optional, and which ones I need to specify?” This is where the documentation comes in handy. You can type a question mark in front of the function you’d like to learn more about (?factor). If you’re using RStudio, this will bring up the documentation page for that function under the help tab in the lower right pane, which will usually include a description, the arguments (and their descriptions), and example code showing how to use the function. You can also type the function into the search bar under the help tab on the lower right pane. Hopefully, though, we’ll provide enough example code throughout this book that you won’t need to concern yourself with all of these details. But, we can’t provide examples of everything for every situation, and you’ll almost certainly use functions that aren’t covered in this book. This is where having a basic understanding of these topics will come in handy. Code Formatting As a final note, just know that the spacing in your code doesn’t matter. You can type 5 * 5 or 5*5 and get the same result. You can type mean(1:10) or mean(1 : 10) and you’ll get the same result. You can even type mean (1:10) or mean (1 : 10), but you probably shouldn’t because this is very unconventional. "],
["packages-and-libraries.html", "3 Packages and Libraries What are Packages? Installing Packages What’s a Library? Loading Packages", " 3 Packages and Libraries What are Packages? R has hundreds of useful functions, but sometimes you’ll want or need to use a function that’s not built into R. For example, R has several built-in functions for plotting data, such as the hist() function: hist(rnorm(100)) And that works fine if you want to quickly analyze your data, but that graph is certainly not publication quality. You could spruce up the plot by installing a package that contains functions for creating much more aesthetically pleasing plots, such as the ggplot2 package. library(ggplot2) set.seed(1) ggplot(tibble::tibble(rnorm = rnorm(100, mean = -1)), aes(x = rnorm)) + geom_density(tibble::tibble(rnorm = rnorm(100, mean = 1)), mapping = aes(x = rnorm), fill = &quot;blue&quot;, alpha = .7) + geom_density(fill = &quot;red&quot;, alpha = .7) + theme_bw() + labs(title = &quot;A Colorful Distribution Example&quot;) + xlab(&quot;Random Normal Distribution&quot;) + ylab(&quot;Density&quot;) + theme(panel.grid.minor.y = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_blank(), text = element_text(size = 16, family = &quot;serif&quot;)) The ggplot2 package is simply a collection of R code and functions, created by an R user, that allows you to make highly customizable plots like the one above without too much difficulty. Installing Packages There are many packages that you can install to make data analysis easier. One package that we’ll use frequently throughout this book is the tidyverse package. The tidyverse package is actually a collection of packages bundled together– a meta-package. You can install the tidyverse with the following line of code: install.packages(&quot;tidyverse&quot;) # You only need to install a package once What’s a Library? Once you install a package it becomes part of your “library”. Your library is a collection of all the packages that are installed on your computer. If you click on the “packages” tab in the lower right pane, you can see a list of all the packages that are currently installed. This is your library. Notice how the header says “System Library”. Loading Packages Packages only need to be installed once, but they need to be loaded every time you start an R session. Loading a package puts it into the working memory; that way R knows you want to use those functions from those packages. You can load a package with the library() function: library(tidyverse) Running the code library(tidyverse) now allows you to use all of the functions that come bundled with the tidyverse package (which is a lot!). Pro tip: It’s best practice to load your desired package with the library() function at the very beginning of your R script; that way it’s loaded before the rest of your code is ran. You can check to see if you installed the tidyverse package and loaded it into your R session correctly by running the ggplot histogram code above. If you see the same plot in your R session, everything is working correctly! You might be wondering why the ggplot code worked when we loaded the tidyverse package and not the ggplot2 package. This is because the tidyverse is a meta-package, which includes the ggplot2 package within it (along with 7 other packages: dplyr, tidyr, readr, purr, tibble, stringr, forcats). So, if you get into the habit of loading the tidyverse meta-package at the beginning of your script, that will generally take care of everything for you, and you won’t need to worry about loading many (if any) other packages into your R session. "],
["CH4.html", "4 Reading and Writing Data Setting the Working Directory Reading Data Writing Data", " 4 Reading and Writing Data In this chapter we’ll work through a few examples of reading and writing data. If you’d like to follow along with the examples, you can download the folder for this book here. After downloading and unzipping the folder, open the file that ends with the extension .Rproj. This will set your initial working directory to the correct folder, which is important for following along in the section below. Setting the Working Directory Before you start importing files, you need to set the working directory so R looks for and saves files in the correct place. “Directory” is a synonym for folder, so ‘setting the working directory’ really just means we need to tell R which folder we’d like to use to import data from. You can determine your current working directory with the getwd() function: getwd() When you run this code, you should have a file path printed to your console. This file path is your current working directory, which means that’s where R is currently looking for files. More specifically, the last folder name listed in the file path is where R is looking for files. To change the working directory, you simply need to navigate to the folder you’d like to import files from in the lower right pane under the files tab. There are two folders shown in the example image above. If you’re following along with the folder we created for this book, then your folder setup should look the same. To import files from the data folder, you would click on the data folder and then click on the More dropdown arrow and click Set as Working Directory. After doing this, you should see something in your console similar to this: setwd(\"~/Your/File/Path\"), which indicates that your working directory has been changed to the listed file path. If you type and run the code list.files() you can see the names of the files that are in your current working directory. If you’re following along with the folder provided, you should see several data files listed when you run this code, such as the BMI_1.csv. Changing the working directory only temporarily changes it for your current R session. It does not permanently change it. Fir this reason, it’s a good idea to add the code setwd(\"~/Your/File/Path\"), which is the same code that’s printed to the console, to the beginning of your R script so that you do not have to manually set your working directory every time you open your R script. Reading Data The code in the rest of this chapter will only work if you are following along with the folder for this book and have correctly set the working directory to the data folder (see above). Once the working directory has correctly been set, importing data files is easy. There’s a file in the data folder named BMI_1.csv. A .csv file is a comma separated value file, which, as the name implies, consists of values separated by a comma. We can import the BMI_1.csv file with the read.csv() function: data_csv &lt;- read.csv(&quot;BMI_1.csv&quot;) # Don&#39;t forget to assign the csv file to an object And if the BMI_1.csv was instead a .txt file (tab delimited file) or .xlsx / .xls (Excel) file, we could use the read.delim() and read_excel() functions, respectively: # txt file data_txt &lt;- read.delim(&quot;BMI_1.txt&quot;) # excel file data_xlsx &lt;- read_excel(&quot;BMI_1.xlsx&quot;) # You need to load the tidyverse package to use the read_excel() function read_excel() is a function that comes from the tidyverse package, which means you need to load the tidyverse into your R session in order to use it (library(tidyverse)). After you’ve imported your data you can view it with the head() function: head(data_xlsx) # A tibble: 6 x 2 height weight &lt;dbl&gt; &lt;dbl&gt; 1 68.5 155. 2 65.3 174. 3 62.1 138. 4 64.8 165. 5 70.1 149. 6 67.3 197. The three import functions above have many arguments that you can change if the data is not imported in the format you were expecting. For example, txt files can be formatted in different ways; sometimes the data is delimited with a tab, or it may be delimited with a semicolon. If your txt file is not separated by a tab, you can change the sep argument (which is short for separator) to indicate how it is separated (the default argument for sep is \"\\t\", which means separated by a tab). Here’s an example where the txt file is not delimited by a tab, so it gets imported incorrectly: data &lt;- read.delim(&quot;BMI_2.txt&quot;) head(data) height.weight 1 68.5166834855604;154.681763861273 2 65.3186507959108;173.958957660404 3 62.0789448822327;138.211660043739 4 64.8491663241866;165.020083309111 5 70.1389452245124;148.630607472854 6 67.3137063327876;196.627466047489 We can fix this by and passing the sep argument (which is short for separator) a semicolon, which indicates that the data is not delimited with a tab, but instead with a semicolon: data &lt;- read.delim(&quot;BMI_2.txt&quot;, sep = &quot;;&quot;) head(data) height weight 1 68.51668 154.6818 2 65.31865 173.9590 3 62.07894 138.2117 4 64.84917 165.0201 5 70.13895 148.6306 6 67.31371 196.6275 As another example, you could read data from the third sheet of an excel file and skip the first 3 lines by specifying the sheet and skip arguments: read_excel(your_file, sheet = 3, skip = 3) Writing Data Writing files is just as easy as reading files. You can write .txt and .csv files with the following functions: # Write .txt file write.table(x = data, file = &quot;data.txt&quot;, sep = &quot;/t&quot;) # Write .csv file write.csv(x = data, file = &quot;data.csv&quot;) The first argument, x, is an R object, which is typically your dataset object. The second argument, file, is what you’d like to name the file you’re creating. The data file will be saved in whatever folder you’re currently in. "],
["CH5.html", "5 Describing and Visualizing Data Viewing Data Data Summaries Visualizing Data", " 5 Describing and Visualizing Data In this chapter we’ll work through a few examples of viewing, summarizing, and visualizing data. We’ll avoid importing files to make it as easy as possible to follow along. Viewing Data First, we’ll read in the powerlifting.csv file in the data folder. data &lt;- read.csv(&quot;power.csv&quot;) After running the code above, you should see an object called data under the Environment tab in the upper right pane of RStudio. If you click on the white grid icon, RStudio will display the data in tabular form. In this tabular view you can filter columns, arrange columns by ascending or descending values, and search values. Another way to get the feel for an entire dataset is with the str() function. The benefit of the str() function is that the variable type is also displayed. str(data) &#39;data.frame&#39;: 24 obs. of 6 variables: $ subject : int 1 2 3 4 5 6 7 8 9 10 ... $ sex : chr &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ... $ self_ID : chr &quot;endurance&quot; &quot;power&quot; &quot;power&quot; &quot;endurance&quot; ... $ mass_kg : num 65 80 71 64.5 73 ... $ VJ_power_watts: num 4203 5332 4431 3256 4001 ... $ WG_power_watts: int 809 995 856 595 707 956 870 966 1100 1188 ... If you’re only interested in viewing the first few rows or last few rows of data you can use the head() and tail() functions, respectively: head(data) # First 6 rows tail(data) # Last 6 rows subject sex self_ID mass_kg VJ_power_watts WG_power_watts 1 1 M endurance 65.00000 4203.498 809 2 2 M power 80.00000 5332.476 995 3 3 M power 71.00000 4430.985 856 4 4 M endurance 64.50000 3256.266 595 5 5 M endurance 73.00000 4001.481 707 6 6 M power 81.81818 5289.559 956 subject sex self_ID mass_kg VJ_power_watts WG_power_watts 19 19 F power 48.00000 3306.285 540 20 20 F power 75.00000 3769.716 618 21 21 F power 75.90909 3286.867 752 22 22 F endurance 55.90909 2572.519 622 23 23 F endurance 50.00000 2283.564 492 24 24 F power 70.45455 4140.660 951 Sometimes it’s useful/convenient to access certain attributes of the data frame, such as the column names. A few of the functions for accessing attributes are provided below. dim(data) # Dimensions nrow(data) # Number of rows ncol(data) # Number of columns colnames(data) # Column names Data Summaries One of the quickest and most useful ways to produce a data summary is with the summary() function. This function is versatile and works well with both continuous and categorical data: summary(data) subject sex self_ID mass_kg Min. : 1.00 Length:24 Length:24 Min. :45.45 1st Qu.: 6.75 Class :character Class :character 1st Qu.:67.39 Median :12.50 Mode :character Mode :character Median :72.75 Mean :12.50 Mean :71.72 3rd Qu.:18.25 3rd Qu.:79.66 Max. :24.00 Max. :94.09 VJ_power_watts WG_power_watts Min. :2284 Min. : 373.0 1st Qu.:3354 1st Qu.: 621.0 Median :4234 Median : 849.0 Mean :4197 Mean : 805.5 3rd Qu.:4927 3rd Qu.: 939.8 Max. :6087 Max. :1188.0 For R to correctly interpret the categorical variables in this data frame, like sex and self_ID, the variables must be of the factor variable type. Currently these variables are listed as character. They can be converted to factors with the as.factor() function. We can then use the summary() function after converting the variable. summary(as.factor(data$sex)) summary(as.factor(data$`self_ID`)) F M 8 16 endurance power 11 13 You can also create a contingency table of the counts at each combination of factor levels with the table() function: table(data$sex, data$self_ID) endurance power F 3 5 M 8 8 More than two variables can be used, too. Similarly, you can create conditional proportions by passing the table created to the prop.table() function: prop.table(table(data$sex, data$self_ID)) endurance power F 0.1250000 0.2083333 M 0.3333333 0.3333333 Below are some more descriptive functions, and you can probably guess what each of them does. mean() median() var() sd() min() max() range() quantile() colSums() rowSums() colMeans() rowMeans() Missing Data In the examples above, none of the data contained missing values, which made the data summaries easy to calculate. Missing data can be handled in many different ways, and selecting the most appropriate method is beyond the scope of this book. Instead, we’ll only cover how to find and omit observations with missing data. First we’ll create a data frame in R that contains NA values: data2 &lt;- data.frame(Col1 = c(NA, 1, 21, 34, NA), Col2 = c(23, 34, 34, 12, 56), Col3 = c(NA, 2, 12, 43, 12)) data2 Col1 Col2 Col3 1 NA 23 NA 2 1 34 2 3 21 34 12 4 34 12 43 5 NA 56 12 You can use the is.na() function to assess if a value is missing in a data frame or not. For example: is.na(data2) # Missing values in data frame Col1 Col2 Col3 [1,] TRUE FALSE TRUE [2,] FALSE FALSE FALSE [3,] FALSE FALSE FALSE [4,] FALSE FALSE FALSE [5,] TRUE FALSE FALSE What you get in return is TRUE and FALSE values. If the value is TRUE, that means there is an NA value in that location. This might not be particularly helpful, though, if you have a large data frame. Instead, it might be more useful to have the count of NA values returned. To do this, we could use the is.na() function wrapped inside of a sum() function: sum(is.na(data2)) # Sum of missing values in data frame [1] 3 You might want to know how many NA values are in a specific column rather than the entire dataset. Selecting just one column from a data frame is a very common task, and one way you can do this is with the dollar sign, $: sum(is.na(data2$Col1)) # Sum of missing values in Col1 [1] 2 A quick way to get the missing values for all columns of a data frame is with the colSums() function: colSums(is.na(data2)) Col1 Col2 Col3 2 0 1 You can omit all missing values in a data frame with the na.omit() function: na.omit(data2) Col1 Col2 Col3 2 1 34 2 3 21 34 12 4 34 12 43 The “opposite” of the na.omit() function would be finding and listing all the rows that do contain missing values, and that’s what the code below accomplishes: data2[!complete.cases(data2), ] Col1 Col2 Col3 1 NA 23 NA 5 NA 56 12 Recall from Chapter 2 that brackets ,[], are used to subset in R. For example, if you wanted the 5th row and 3rd column returned from a data frame, you could do this by subsetting the data frame like so: data[5,3]. The first item within the bracket refers to the row number, and the second number refers to the column number. If you don’t put any numbers within the brackets, data[,], then everything is selected (the entire data frame). Going back to the example above, you can see that data2 is being subsetted by the rows. complete.cases() is a function similar to na.omit(), where the NA values are not included. Notice that there is an exclamation point, !, in front of the function. This means that instead of returning the cases that are complete, what’s being returned is the cases that are not complete from the data frame. So, we’re subsetting data2 by its rows, where the incomplete cases are returned, and all the columns are selected. If that was confusing, that’s okay! You might have noticed that you can select a column in R like this: data$Col3, and also like this data[ ,3]. There are almost always multiple ways to accomplish something in R, and both ways work equally well. Let’s try computing the mean for the first column of the data2 object which contains missing values: mean(data2$Col1) [1] NA As you can see, NA is returned. This is because Col1 contains missing values. Luckily the mean() function has an na.rm argument, which stands for ‘not available, remove’. We can set this argument to TRUE, so that missing values will be removed when computing the mean: mean(data2$Col1, na.rm = TRUE) [1] 18.66667 Visualizing Data There are entire books written about visualizations in R. In fact, there are entire books written about visualizations in R using one specific package! Here, we just wanted to cover a few types of plots and plotting features that can hopefully be of use when analyzing data. Histogram You can make histograms with the hist() function. All you need to do is select the column to plot, which in this case is data$VJ_power_watts, and the rest will be done for you: hist(x = data$VJ_power_watts) You could then change the color and labels, create a title, add vertical lines indicating the mean and median (which can each have their own color), change the bins, edit the axes, change the border color, and much more. This is just to give you an idea, in case you’d like to create more complete plots in R. hist(x = data$VJ_power_watts, col = &quot;lightblue&quot;, xlab = &quot;Vertical Jump Power (Watts)&quot;, main = &quot;Vertical Jump Power&quot;, border = &quot;black&quot;) abline(v = mean(data$VJ_power_watts), col = &quot;red&quot;) abline(v = median(data$VJ_power_watts), col = &quot;black&quot;) Scatterplot You can use the plot() function to make scatter plots or various other types of plots. Simply pass your data frame object to the function and R will plot all of the relationships. plot(x = data) Rather than using all of the data, you can select the relevant variables by subsetting: plot(data[4:6]) In the code above, the columns 4:6 were selected, which are the columns shown in the plot. You could then color the points by a categorical variable, such as the self_ID variable, which is a variable indicating whether a subject self-identifies as more of a power or endurance athlete. Remember that in order for this to work, the self_ID column needs to be converted to a factor. plot(data[4:6], col = as.factor(data$self_ID)) If you wanted to plot the bivariate relationship between two of the variables, you would simply select two of the columns instead of three. Similar to the vertical lines in the histogram plot, you can add the slope and intercept to a plot with the same abline() function. These values were created by performing a linear model and then grabbing the coefficients from the model. plot(data[4:5]) intercept &lt;- lm(VJ_power_watts ~ mass_kg, data = data)$coeff[1] slope &lt;- lm(VJ_power_watts ~ mass_kg, data = data)$coeff[2] abline(coef = c(intercept, slope), col = &quot;red&quot;,) Barplot You can make bar graphs by using the table() function used earlier in conjunction with the barplot() function. barplot(table(data$self_ID)) barplot(table(data$self_ID, data$sex)) You can have the bars side-by-side instead of stacked by setting beside = TRUE. Notice that you can also generate legends for your plots with the legend() function and then coloring them accordingly. barplot(table(data$sex, data$self_ID), beside = TRUE, col = c(&quot;lightcoral&quot;, &quot;lightblue&quot;)) legend(&quot;topleft&quot;, c(&quot;F&quot;, &quot;M&quot;), fill = c(&quot;lightcoral&quot;, &quot;lightblue&quot;)) Boxplot Boxplots can be created with the boxplots() function. The formatting for this function may seem a little strange. For the formula argument, you’ll first list the y variable followed the x variables. The x variables will be separted by a plus sign. This function automatically converts the sex and self_ID columns to factors for us. boxplot(formula = VJ_power_watts ~ self_ID + sex, data = data, col = c(&quot;lightcoral&quot;, &quot;lightgoldenrod&quot;, &quot;lightblue&quot;, &quot;lightgreen&quot;)) You can view the list of available color options for plots by typing colors() into the console. "],
["CH6.html", "6 Formatting Data Merging Files Long Format Wide Format Recoding Variables Changing Variable Types Rename Columns Change Column Order Data Manipulation", " 6 Formatting Data This chapter will cover: merging data files, long and wide format, recoding varibles, changing variable types, column renaming and arrangement, and data manipulation. The intention is to briefly cover some of the most common tasks that are performed in R. There are many resources that cover these topics and more in much greater depth (and with better explanation), such as Roger D. Peng’s R Programming for Data Science and R for Data Science by Garrett Grolemund and Hadley Wickham. The code in this chapter only works if you’re following along with the Github folder for this book (which you can download here), correctly set your working directory to the data folder (which you can learn how to do in Chapter 4), and run the code in the order it appears in this chapter. Merging Files Sometimes you might want to import multiple files and combine them into one dataset. This can be accomplished with the cbind() and rbind() functions, which are short for column bind and row bind. Let’s say you have three files that have the same column names and you want to combine them. data1 &lt;- read.csv(&quot;BMI_1.csv&quot;) dim(data1) data2 &lt;- read.csv(&quot;BMI_2.csv&quot;) dim(data2) data3 &lt;- read.csv(&quot;BMI_3.csv&quot;) dim(data3) newData &lt;- rbind(data1, data2, data3) dim(newData) [1] 20 2 [1] 20 2 [1] 20 2 [1] 60 2 As you can see the last printout shows the dimensions are 60 rows by 2 columns, which means the three data frames were successfully combined. Let’s say you have two files with different column names and want to combine them side-by-side. For example, you might have a data file that contains subID, age and sex, and another file contains height and weight data. These columns can be added into one dataset with the cbind() function. subID, age, and sex dataset: data1 &lt;- read.csv(&quot;SubID_Age_Sex.csv&quot;) head(data1) subID age sex 1 1 30.68838 F 2 2 28.33384 M 3 3 33.65584 M 4 4 35.51969 M 5 5 32.43415 M 6 6 28.14712 F height and weight dataset: data2 &lt;- read.csv(&quot;BMI_1.csv&quot;) head(data2) height weight 1 68.51668 154.6818 2 65.31865 173.9590 3 62.07894 138.2117 4 64.84917 165.0201 5 70.13895 148.6306 6 67.31371 196.6275 Combind with the cbind() function: newData &lt;- cbind(data1, data2) head(newData) subID age sex height weight 1 1 30.68838 F 68.51668 154.6818 2 2 28.33384 M 65.31865 173.9590 3 3 33.65584 M 62.07894 138.2117 4 4 35.51969 M 64.84917 165.0201 5 5 32.43415 M 70.13895 148.6306 6 6 28.14712 F 67.31371 196.6275 It is important to note that the rbind() function is only useful when binding two datasets that have all of the same column names and they’re in the same order. Similarly, the cbind() function is only useful when combining data with the same number of rows. If this is not the case, you can use the merge function for more complex joining operations. Type merge into the search bar under the help tab in RStudio to learn more about the function, or run the code ?base::merge. Long Format Putting data in long format increases the number of rows and decreases the number of columns in a data frame. In the health sciences, this often means that one row corresponds to one observation (at one point in time). If your data is not in long format, you can change it to long format with the pivot_longer() function, which comes from the tidyverse package. Here’s an example of a research study where subjects had their VO2 max measured at four points in time, and it’s currently not in long format: data_VO2 &lt;- read.csv(&quot;data_VO2.csv&quot;) head(data_VO2) SubID Time1 Time2 Time3 Time4 1 1 36.72302 34.08948 41.50392 42.85447 2 2 28.09261 33.22172 39.22484 47.51721 3 3 25.32283 34.95023 38.91351 40.67717 4 4 31.87634 35.51951 38.05794 46.51975 5 5 30.98121 33.18143 41.16101 46.96758 6 6 31.83277 35.58868 33.92712 45.27417 We can convert the data_VO2.csv into long format with the pivot_longer() function: # Make sure the tidyverse package is loaded data_VO2_long &lt;- pivot_longer(data = data_VO2, cols = c(&quot;Time1&quot;, &quot;Time2&quot;, &quot;Time3&quot;, &quot;Time4&quot;), names_to = &quot;Time&quot;, values_to = &quot;VO2_max&quot;) head(data_VO2_long) # A tibble: 6 x 3 SubID Time VO2_max &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 Time1 36.7 2 1 Time2 34.1 3 1 Time3 41.5 4 1 Time4 42.9 5 2 Time1 28.1 6 2 Time2 33.2 Wide Format Putting data in wide format increases the number of columns in a data frame. In the health sciences, this often means that one row will contain multiple points in time; this is how the data_VO2.csv was formatted before we converted it to long format. We can now change it back into wide format with the pivot_wider() function, which also comes from the tidyverse package. # Make sure the tidyverse package is loaded data_VO2_wide &lt;- pivot_wider(data = data_VO2_long, names_from = &quot;Time&quot;, values_from = &quot;VO2_max&quot;) head(data_VO2_wide) # A tibble: 6 x 5 SubID Time1 Time2 Time3 Time4 &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 36.7 34.1 41.5 42.9 2 2 28.1 33.2 39.2 47.5 3 3 25.3 35.0 38.9 40.7 4 4 31.9 35.5 38.1 46.5 5 5 31.0 33.2 41.2 47.0 6 6 31.8 35.6 33.9 45.3 Recoding Variables One of the easiest ways to recode/fix factors in R is with the factor() function. We’ll use the SubID_Age_Sex.csv dataset that was used earlier: SubID_Age_Sex &lt;- read.csv(&quot;SubID_Age_Sex.csv&quot;) head(SubID_Age_Sex) subID age sex 1 1 30.68838 F 2 2 28.33384 M 3 3 33.65584 M 4 4 35.51969 M 5 5 32.43415 M 6 6 28.14712 F For the factor() function, the first argument, x, is the column that should be selected, which in this case is sex. Again, we can use the dollar sign $ to select a specific column from a dataset. The second argument, levels, is the levels that should be included, and the third argument, labels, is the labels that should be assigned to each level, respectively. SubID_Age_Sex$sex &lt;- factor(x = SubID_Age_Sex$sex, levels = c(&quot;M&quot;,&quot;F&quot;), labels = c(0,1)) head(SubID_Age_Sex) subID age sex 1 1 30.68838 1 2 2 28.33384 0 3 3 33.65584 0 4 4 35.51969 0 5 5 32.43415 0 6 6 28.14712 1 Let’s take a look at a recoding example that’s slighly more involved. We’ll use the data_VO2.csv dataset used earlier. data_VO2 &lt;- read.csv(&quot;data_VO2_long.csv&quot;) head(data_VO2) SubID Time VO2_max 1 1 Time1 24.75065 2 1 Time2 35.03364 3 1 Time3 42.41792 4 1 Time4 46.61252 5 2 Time1 31.29356 6 2 Time2 29.43164 And we’ll add a column of the subject’s sex: data_VO2$Sex &lt;- rep(c(rep(&quot;M&quot;, 4), rep(&quot;F&quot;, 4)), 10) head(data_VO2) SubID Time VO2_max Sex 1 1 Time1 24.75065 M 2 1 Time2 35.03364 M 3 1 Time3 42.41792 M 4 1 Time4 46.61252 M 5 2 Time1 31.29356 F 6 2 Time2 29.43164 F Rather than having VO2max as continuous data, we’ll change it to categorical data: poor, fair, good, excellent. The catch, however, is that the classification will partially depend on whether the subject was a male or female (it also depends on age, but we’ll assume everyone in the study was 30 years old). The easiest way to do this is with the ifelse() function, and we’ll also use several functions from the tidyverse package. First, we need to filter the data so that only males or females are included in the dataset, but not both. We can do this with the filter() function from the tidyverse package. We’ll also need to create a new column to store the data in, and one easy way to do this is with the mutate() function from the tidyverse package. data_VO2_F &lt;- filter(data_VO2, Sex == &quot;F&quot;) data_VO2_F &lt;- mutate(data_VO2, VO2_max_cat = ifelse(VO2_max &lt; 22.8, &quot;Very Poor&quot;, ifelse(VO2_max &gt;= 22.8 &amp; VO2_max &lt;= 26.9, &quot;Poor&quot;, ifelse(VO2_max &gt;= 27.0 &amp; VO2_max &lt;= 31.4, &quot;Fair&quot;, ifelse(VO2_max &gt;= 31.5 &amp; VO2_max &lt;= 35.6, &quot;Good&quot;, ifelse(VO2_max &gt;= 35.7 &amp; VO2_max &lt;= 40.0, &quot;Excellent&quot;, &quot;Superior&quot;)))))) head(data_VO2_F) SubID Time VO2_max Sex VO2_max_cat 1 1 Time1 24.75065 M Poor 2 1 Time2 35.03364 M Good 3 1 Time3 42.41792 M Superior 4 1 Time4 46.61252 M Superior 5 2 Time1 31.29356 F Fair 6 2 Time2 29.43164 F Fair In the code above the data were filtered to include only females. Notice that there are two equals signs, ==, within the filter() function. This is because two equals signs is used to represent a logical statement. The first line of code above is filtering the data to only include females: Sex == \"F\". This is a logical statement where we are saying we only want to include the data where Sex == \"F\" is true. If you were to add only one equals sign here you’d get an error, and errors like this can be very difficult to find! After filtering, the data frame was assigned to the object data_VO2_F. Next, the mutate() function was used to create a new column called VO2_max_cat (cat for category). Then, the ifelse() function was used to assign a category based on the subject’s VO2_max. The first line reads, “If the VO2_max is less than 22.8, assign it the value”Very Poor“.” The last category, Superior, is assigned if the value does not meet any of the other conditions, which in this case is all values greater than 40.0. Now we can repeat the same process for males: data_VO2_M &lt;- filter(data_VO2, Sex == &quot;M&quot;) data_VO2_M &lt;- mutate(data_VO2_M, VO2_max_cat = ifelse(VO2_max &lt; 31.5, &quot;Very Poor&quot;, ifelse(VO2_max &gt;= 31.5 &amp; VO2_max &lt;= 35.4, &quot;Poor&quot;, ifelse(VO2_max &gt;= 35.5 &amp; VO2_max &lt;= 40.9, &quot;Fair&quot;, ifelse(VO2_max &gt;= 41.0 &amp; VO2_max &lt;= 44.9, &quot;Good&quot;, ifelse(VO2_max &gt;= 45.0 &amp; VO2_max &lt;= 49.4, &quot;Excellent&quot;, &quot;Superior&quot;)))))) head(data_VO2_M) SubID Time VO2_max Sex VO2_max_cat 1 1 Time1 24.75065 M Very Poor 2 1 Time2 35.03364 M Poor 3 1 Time3 42.41792 M Good 4 1 Time4 46.61252 M Excellent 5 3 Time1 28.31473 M Very Poor 6 3 Time2 32.42022 M Poor If you wanted the data frames to be combined back into one data frame that included both males and females, you could use the rbind() function. You might want to recode certain values as NA if you know the observed values are impossible. Here’s an example where Age data is collected, and values less than 0 or greater than 130 are recoded as NA. Notice the vertical bar in the code. The vertical bar means or. So the code below reads, “If Age is less than 0 or greater than 130, assign the value NA to it.” df &lt;- data.frame(Age = c(1546, 43, 23, 56, -64)) df$Age[df$Age &lt; 0 | df$Age &gt; 130] &lt;- NA df Age 1 NA 2 43 3 23 4 56 5 NA Changing Variable Types As discussed briefly in Chapter 2, there are several variable types in R, and the variable type of the data will affect the operations that you can and cannot perform on that data. For example, if the data is of the character class, you cannot calculate the mean with the mean() function. Let’s say you have 10 values consisting of zeros and ones, where the ones are intended to represent TRUE and zeros are intended to represent FALSE. values &lt;- c(0, 0, 1, 0, 1, 1, 1, 1, 0, 0) Unfortunately, R has no way of knowing that you want the zeros and ones to represent FALSE and TRUE. R will initially interpret the values exactly as they are, as numeric zeros and ones. You can find out the class of the object with the class() function: class(values) [1] &quot;numeric&quot; As expected, the values are numeric. We can coerce the values to the logical class, so that the zeros are FALSE and ones are TRUE: as.logical(values) [1] FALSE FALSE TRUE FALSE TRUE TRUE TRUE TRUE FALSE FALSE Now let’s check the class of the object: class(values) [1] &quot;numeric&quot; Notice that the class is still the same. That’s because we didn’t overwrite the values object, which is what we need to do in order to permanently change the object: values &lt;- as.logical(values) class(values) [1] &quot;logical&quot; R can only perform a coercion that makes sense. words &lt;- &quot;Can this sentence be coverted to a different class?&quot; as.numeric(words) as.integer(words) as.logical(words) as.factor(words) [1] NA [1] NA [1] NA [1] Can this sentence be coverted to a different class? Levels: Can this sentence be coverted to a different class? As you can see in the example above, the sentence (which is of the character class) could not successfully be converted to a numeric, integer, or logical class. Doing so produced NAs in each case. The sentence could, however, be converted to a factor class. Although not at all sensible in this situation, it is possible because the string of characters can be represented as a categorical variable. Here’s one more example using the mtcars data frame, which consists of 11 columns that are all of the numeric class: str(mtcars) &#39;data.frame&#39;: 32 obs. of 11 variables: $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... $ disp: num 160 160 108 258 360 ... $ hp : num 110 110 93 110 175 105 245 62 95 123 ... $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... $ wt : num 2.62 2.88 2.32 3.21 3.44 ... $ qsec: num 16.5 17 18.6 19.4 17 ... $ vs : num 0 0 1 1 0 1 0 1 1 1 ... $ am : num 1 1 1 0 0 0 0 0 0 0 ... $ gear: num 4 4 4 3 3 3 3 4 4 4 ... $ carb: num 4 4 1 1 2 1 4 2 2 4 ... It would make more sense to represent the cyl (cylinder) as a factor in most situations, since the data are not continuous and there are only 3 possible cylinder sizes in the data frame: mtcars$cyl &lt;- as.factor(mtcars$cyl) mtcars$cyl [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4 Levels: 4 6 8 Rename Columns Renaming columns is a very common task. Before renaming the columns, you might want to print all of the column names to the screen so you can look at them. This can be done with the colnames() function: colnames(mtcars) [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; [11] &quot;carb&quot; Let’s say that we wanted to be abundantly clear about the column names of the mtcars data frame. Instead of mpg as the first column name, let’s rename it to miles_per_gallon. colnames(mtcars)[1] &lt;- &quot;miles_per_gallon&quot; colnames(mtcars) [1] &quot;miles_per_gallon&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; [5] &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; [9] &quot;am&quot; &quot;gear&quot; &quot;carb&quot; The brackets were used to select the first item of column names, which in this case was mpg. If you wanted to rename multiple columns, you could do this in one step: colnames(mtcars)[c(2, 4, 6)] &lt;- c(&quot;cylinder&quot;, &quot;horsepower&quot;, &quot;weight&quot;) colnames(mtcars) [1] &quot;miles_per_gallon&quot; &quot;cylinder&quot; &quot;disp&quot; &quot;horsepower&quot; [5] &quot;drat&quot; &quot;weight&quot; &quot;qsec&quot; &quot;vs&quot; [9] &quot;am&quot; &quot;gear&quot; &quot;carb&quot; Columns 2, 4, and 6 were selected with the concatenate function (c()) and the names cylinder, horsepower, and weight were assigned to those column names, respectively. Change Column Order You can change the order of columns in a data frame by subsetting with brackets and then using the concatenate function to change the order, like so: iris &lt;- iris[c(5, 1:4)] iris The fifth column in the iris data frame, Species, is now listed first instead of last. Data Manipulation In this section we’ll provide a few examples of data transformation using the tidyverse package. Make sure you have the tidyverse library loaded before using the functions listed below: select(): pick variables (columns) by their names filter(): select observations by their values mutate(): create new variables (column) with functions of existing variables arrange(): Reorder the rows summarise(): collapse many values down to a single summary group_by(): change the scope of a function to operate on a group In the Recoding Variables section above we’ve already seen how data can easily be filtered with the filter() function from the tidyverse package. There might be situations where you need to do more than simply filtering the data. Maybe you’d like to filter the data, group it by some grouping variable, and then summarize the data based off of the grouping and filtering. This, and much more, can be done with some of the functions in the tidyverse package. Let’s look at an example where we’d like to perform several transformations to the BMI_1.csv and SubID_Age_Sex.csv data frames. Earlier we combined these data frames into a new object called newData, which is the object we’ll be manipulating. Make sure to run the code below if you haven’t already. data1 &lt;- read.csv(&quot;SubID_Age_Sex.csv&quot;) data2 &lt;- read.csv(&quot;BMI_1.csv&quot;) newData &lt;- cbind(data1, data2) head(newData) subID age sex height weight 1 1 30.68838 F 68.51668 154.6818 2 2 28.33384 M 65.31865 173.9590 3 3 33.65584 M 62.07894 138.2117 4 4 35.51969 M 64.84917 165.0201 5 5 32.43415 M 70.13895 148.6306 6 6 28.14712 F 67.31371 196.6275 Let’s say we wanted to calculate the BMI of males only, and then arrange the data so that the highest BMI values are listed first. First we’d need to filter the data to include only males, then convert height and weight to metric values, then calculate BMI, then arrange the data in descending order according to BMI. We’ll use the filter(), mutate(), and arrange() functions to do this. Good Option Here is one option of how the desired transformation could be accomplished. The code is readable, but several objects had to be created as intermediate steps, which is not ideal. newData1 &lt;- filter(newData, sex == &quot;M&quot;) newData2 &lt;- mutate(newData1, weight_kg = weight / 2.2, height_m = height * .0254, bmi = weight_kg / height_m^2) newData3 &lt;- arrange(newData2, desc(bmi)) newData3 subID age sex height weight weight_kg height_m bmi 1 14 31.20603 M 63.05955 173.9793 79.08148 1.601712 30.82518 2 2 28.33384 M 65.31865 173.9590 79.07225 1.659094 28.72647 3 4 35.51969 M 64.84917 165.0201 75.00913 1.647169 27.64636 4 19 29.41172 M 68.84346 184.8655 84.02978 1.748624 27.48150 5 3 33.65584 M 62.07894 138.2117 62.82348 1.576805 25.26771 6 10 25.18445 M 71.39754 177.2335 80.56069 1.813498 24.49567 7 7 30.53586 M 71.15408 166.7169 75.78042 1.807314 23.20010 8 5 32.43415 M 70.13895 148.6306 67.55937 1.781529 21.28628 This way of doing things is just fine, but it’s not very efficient. Data manipulation often involves many steps, and it would be quite the hassle to create a new object for every step in the process, and then use that new object for further manipulation. Thankfully, there’s a better way to do this. Better Option The code below is as readable as the last, and new objects did not have to be created in the process, making it the better option. In this code, the data was filtered to include only males, weight_kg and height_m columns were added, bmi was then calculated using those two columns, and the data was arranged so that the highest BMI values were listed first. newData %&gt;% filter(sex == &quot;M&quot;) %&gt;% mutate(weight_kg = weight / 2.2, height_m = height * .0254, bmi = weight_kg / height_m^2) %&gt;% arrange(desc(bmi)) subID age sex height weight weight_kg height_m bmi 1 14 31.20603 M 63.05955 173.9793 79.08148 1.601712 30.82518 2 2 28.33384 M 65.31865 173.9590 79.07225 1.659094 28.72647 3 4 35.51969 M 64.84917 165.0201 75.00913 1.647169 27.64636 4 19 29.41172 M 68.84346 184.8655 84.02978 1.748624 27.48150 5 3 33.65584 M 62.07894 138.2117 62.82348 1.576805 25.26771 6 10 25.18445 M 71.39754 177.2335 80.56069 1.813498 24.49567 7 7 30.53586 M 71.15408 166.7169 75.78042 1.807314 23.20010 8 5 32.43415 M 70.13895 148.6306 67.55937 1.781529 21.28628 This was all accomplished without creating new objects thanks to the pipe operator, %&gt;%. Think of the pipe as the word “then”. For example, take the newData object then filter to include only males, then mutate to calculate BMI values, then arrange the data. Here’s another example that utilizes the other three functions: select, group_by, and summarise. We’ll be using the data_VO2 object created earlier. Make sure to run the code below if you haven’t already. data_VO2 &lt;- read.csv(&quot;data_VO2_long.csv&quot;) data_VO2$Sex &lt;- rep(c(rep(&quot;M&quot;, 4), rep(&quot;F&quot;, 4)), 10) head(data_VO2) SubID Time VO2_max Sex 1 1 Time1 24.75065 M 2 1 Time2 35.03364 M 3 1 Time3 42.41792 M 4 1 Time4 46.61252 M 5 2 Time1 31.29356 F 6 2 Time2 29.43164 F In this example, we want to select all columns except for the SubID column, group the observations by Time and Sex, perform a summary statistic where we calculate the average VO2_max based on that grouping, and count the number of observations in each grouping. data_VO2 %&gt;% select(!SubID) %&gt;% group_by(Time, Sex) %&gt;% summarise(VO2_max_avg = mean(VO2_max), count = n()) # A tibble: 8 x 4 # Groups: Time [4] Time Sex VO2_max_avg count &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; 1 Time1 F 30.7 10 2 Time1 M 28.4 10 3 Time2 F 32.0 10 4 Time2 M 34.4 10 5 Time3 F 40.0 10 6 Time3 M 41.2 10 7 Time4 F 44.1 10 8 Time4 M 45.0 10 In the select() function the SubID column was not selected, which is indicated by the exclamation point, !. The data was then grouped by Time and Sex, and the summarise() function was used to create a new column called VO2_max_avg, which was computed as the average VO2_max for each of the unique groups. A count column was also created with the n() function, which counted the observations in each grouping. We’ve shown that the arrange() function can be used to reorder rows of numeric data by descending (or ascending) values; it can also be used to arrange factors, like the Sex column. Here’s the same code as above, except that the data is arranged by Sex instead of Time: data_VO2 %&gt;% select(!SubID) %&gt;% group_by(Time, Sex) %&gt;% summarise(VO2_max_avg = mean(VO2_max)) %&gt;% arrange(Sex) # A tibble: 8 x 3 # Groups: Time [4] Time Sex VO2_max_avg &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 Time1 F 30.7 2 Time2 F 32.0 3 Time3 F 40.0 4 Time4 F 44.1 5 Time1 M 28.4 6 Time2 M 34.4 7 Time3 M 41.2 8 Time4 M 45.0 These data manipulation functions are far more comprehensive than what’s shown here. To see more examples of these functions and in much greater detail, check out the Data Transformation chapter in R for Data Science, which you can read here. "],
["simple-regression.html", "7 Simple Regression Importing Viewing Formatting Modeling", " 7 Simple Regression In this chapter we’ll use the lm() function for fitting a simple regression model with one continuous predictor, which will take the following forms: \\[y_i=\\beta_0+\\beta_1\\left(X1\\right)+\\epsilon_i\\] \\[y_i=\\beta_0+\\beta_1\\left(X1-\\overline{X1}\\right)+\\epsilon_i\\] The top equation is an untransformed model, and the bottom is a mean-centered model. The code in this chapter only works if you’re following along with the Github folder for this book (which you can download here), you’ve correctly set your working directory to the data folder (which you can learn how to do in Chapter 4), and run the code in the order it appears in this chapter. The data used in this chapter is from Kaggle: https://www.kaggle.com/open-powerlifting/powerlifting-database. This dataset is a snapshot of the OpenPowerlifting database as of April 2019. Importing The first step is importing the data. In this example we’ll use the powerlifting.csv file in the data folder, which is a dataset that consists of competitors and their statistics at a Powerlifting competition: data &lt;- read.csv(&quot;powerlifting.csv&quot;) Viewing You can use the str() and head() functions to get the overall “impression” of the dataset. str(data) &#39;data.frame&#39;: 1000 obs. of 15 variables: $ X : int 1 2 3 4 5 6 7 8 9 10 ... $ Name : chr &quot;Abbie Murphy&quot; &quot;Abbie Tuong&quot; &quot;Ainslee Hooper&quot; &quot;Amy Moldenhauer&quot; ... $ Sex : chr &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; ... $ Event : chr &quot;SBD&quot; &quot;SBD&quot; &quot;B&quot; &quot;SBD&quot; ... $ Equipment : chr &quot;Wraps&quot; &quot;Wraps&quot; &quot;Raw&quot; &quot;Wraps&quot; ... $ Age : num 29 29 40 23 45 37 23 35 36 37 ... $ AgeClass : chr &quot;24-34&quot; &quot;24-34&quot; &quot;40-44&quot; &quot;20-23&quot; ... $ Division : chr &quot;F-OR&quot; &quot;F-OR&quot; &quot;F-OR&quot; &quot;F-OR&quot; ... $ BodyweightKg : num 59.8 58.5 55.4 60 104 74 59.8 80.4 108 74.8 ... $ WeightClassKg : chr &quot;60&quot; &quot;60&quot; &quot;56&quot; &quot;60&quot; ... $ Best3SquatKg : num 105 120 NA 105 140 ... $ Best3BenchKg : num 55 67.5 32.5 72.5 80 82.5 70 77.5 100 95 ... $ Best3DeadliftKg: num 130 145 NA 132 170 ... $ TotalKg : num 290 332.5 32.5 310 390 ... $ Place : chr &quot;4&quot; &quot;2&quot; &quot;1&quot; &quot;3&quot; ... The str() function provides a lot of good information. We now know that the dataset was correctly imported as a data frame which consists of 1000 observations of 27 variables, and we know the column names and column variable types. Additionally, the first few observations for each column is printed. But if you’d rather look at the dataset in a more standard format, you can use the head() function to see the first few observations: head(data[1:9]) # Only the first 9 columns are printed here to save space # By default, the first 6 observations of all columns are printed X Name Sex Event Equipment Age AgeClass Division BodyweightKg 1 1 Abbie Murphy F SBD Wraps 29 24-34 F-OR 59.8 2 2 Abbie Tuong F SBD Wraps 29 24-34 F-OR 58.5 3 3 Ainslee Hooper F B Raw 40 40-44 F-OR 55.4 4 4 Amy Moldenhauer F SBD Wraps 23 20-23 F-OR 60.0 5 5 Andrea Rowan F SBD Wraps 45 45-49 F-OR 104.0 6 6 April Alvarez F SBD Wraps 37 35-39 F-OR 74.0 Let’s hypothesize that competitors who weigh more will have a higher TotalKg, which is the sum of the competitor’s most weight lifted on three lifts: Squat, Bench Press, and Deadlift. First, let’s visualize the relationship between these variables. plot(data$BodyweightKg, data$TotalKg) It looks like data points are concentrated more heavily at specific body weights. This is likely because powerlifters compete in weight classes, and its advantageous to be as close as possible to the cutoff weight limit. We can use the hist() function to visualize the distributions of each variable: par(mfrow = c(1, 2)) hist(data$TotalKg) hist(data$BodyweightKg) The code par(mfrow = c(1,2)) above is used to print the plots as a 1 x 2 grid; the plot() function does not need to be used in conjuction with this piece of code. And you can look at the six number summary with the summary() function. Notice how the summary conveniently includes the NA count. summary(data[c(&quot;TotalKg&quot;, &quot;BodyweightKg&quot;)]) TotalKg BodyweightKg Min. : 32.5 Min. : 46.10 1st Qu.: 341.9 1st Qu.: 72.80 Median : 508.8 Median : 87.90 Mean : 496.7 Mean : 89.45 3rd Qu.: 643.1 3rd Qu.:104.50 Max. :1020.0 Max. :165.00 NA&#39;s :32 More examples of viewing data can be found in Chapter 5 Formatting Before modeling, we need to ask ourselves a few questions. What if someone didn’t perform one of the lifts? Powerlifting competitions have several event options, and we might be including in our analysis individuals who have a TotalKg value, but their value is only for the squat and bench press lifts, rather than all three. Also, what if a competitor didn’t successfully complete a lift? That is, they competed in all three lifts, but they (likely) attempted to lift too much weight and their attempts were unsuccessful so they were disqualified. It probably isn’t a good idea to include either of these scenarios in our analysis because their TotalKg values wouldn’t represent the summation of all three lifts, which is what we’re interested in. We could use the filter() function from the tidyverse package to filter the data and take care of both of these issues in one step. Make sure the tidyverse package is loaded before running this code: # Replace the original data object with the filtered data data &lt;- filter(data, Event == &quot;SBD&quot;, Place != &quot;DQ&quot;) In the code block above, the data was filtered to include only competitors who competed in all three lifts, SBD, and they were not disqualified, DQ, meaning that they had at least one successful lift on all three lifts. This ensures that the TotalKg value is representing the same value for all competitors. More examples of formatting data can be found in Chapter 6 Modeling The lm() Function lm(formula, data, subset, weights, na.action, method = &quot;qr&quot;, model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, offset, ...) The lm() (linear model) function is used for fitting linear models. There are many arguments for this function, but the formula and dataarguments are the only ones that need to be specified. If you’d like to learn more about functions and arguments, Chapter 2 covers basic programming concepts, including functions and arguments. We’ll use the lm() function to make two models: the untransformed linear model and a mean-centered model. Untransformed Model When using the lm() function, the formula argument is set equal to the dependent variable, followed by a tilde, ~, and then the independent variable(s). The data argument is set equal to the object that contains the dataset, which in this example is the object called data. lm(formula = TotalKg ~ BodyweightKg, data = data) Call: lm(formula = TotalKg ~ BodyweightKg, data = data) Coefficients: (Intercept) BodyweightKg 50.83 5.24 The function prints out the slope and intercept for the model. We could then use the slope and intercept to create a plot with an abline: plot(data$BodyweightKg, data$TotalKg) abline(50.83, 5.24, col = &quot;red&quot;) It looks like there may be a significant relationship between the two variables, but how can we be sure? The lm() function printed the coefficients but did not provide information about the R-squared or significance. To see this information, we need to save the model as an object, and then print the summary of the model: my_model &lt;- lm(formula = TotalKg ~ BodyweightKg, data = data) summary(my_model) Call: lm(formula = TotalKg ~ BodyweightKg, data = data) Residuals: Min 1Q Median 3Q Max -553.63 -85.05 0.87 91.00 359.82 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 50.8344 16.7228 3.04 0.00243 ** BodyweightKg 5.2404 0.1815 28.88 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 127.7 on 910 degrees of freedom Multiple R-squared: 0.4782, Adjusted R-squared: 0.4776 F-statistic: 834 on 1 and 910 DF, p-value: &lt; 2.2e-16 Now we have a nice summary print-out which includes the F-statistic, Residuals, R-squared, p-value, and more. What’s also nice is we can now use the plot function on the my_model object that we just created to view Residuals vs Fitted, Normal Q-Q, Scale-Location and Residuals vs Leverage plots. par(mfrow = c(2,2)) plot(my_model) The code par(mfrow = c(2,2)) above is used to print the plots as a 2 x 2 grid; the plot() function does not need to be used in conjuction with this piece of code. What else can be done with the my_model object? Let’s take a look at the object’s attributes: attributes(my_model) $names [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; $class [1] &quot;lm&quot; The model’s attributes can be accessed by using a dollar sign, $. For example, here’s a printout of the first 5 residuals of the model: my_model$residuals[1:5] 1 2 3 4 5 -74.20773 -24.89527 -55.25580 -205.83149 -68.62079 Mean-Centered Model In this example we’ll use the same variables as before, but this time the predictor variable will be mean-centered. First, we create a column that consists of the BodyweightKg mean, which is 89.17. Since there are 1000 rows in the powerlifting dataset, that means we are creating a column that has the value 89.17 repeated 1000 times. This value is then saved into the column BodyweightKg_mean; that’s what the first line of code in the code chunk below is doing. In the second line of code, each BodyweightKg value is subtracted from the BodyweightKg_mean mean column, which is then stored in a new column called BodyweightKg_mc. # Create a column that consists of the mean BodyweightKg data$BodyweightKg_mean &lt;- mean(data$BodyweightKg, na.rm = TRUE) # Subtract BodyweightKg from mean BodyweightKg column data$BodyweightKg_mc &lt;- data$BodyweightKg - data$BodyweightKg_mean Here’s what the new mean-centered column looks like graphically: plot(data$BodyweightKg_mc ,data$TotalKg) abline(lm(data$TotalKg ~ data$BodyweightKg_mc), col = &quot;red&quot;) We can now use this mean-centered column as the dependent variable in the lm() function. my_model2 &lt;- lm(formula = TotalKg ~ BodyweightKg_mc, data = data) summary(my_model2) Call: lm(formula = TotalKg ~ BodyweightKg_mc, data = data) Residuals: Min 1Q Median 3Q Max -553.63 -85.05 0.87 91.00 359.82 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 518.0954 4.2279 122.54 &lt;2e-16 *** BodyweightKg_mc 5.2404 0.1815 28.88 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 127.7 on 910 degrees of freedom Multiple R-squared: 0.4782, Adjusted R-squared: 0.4776 F-statistic: 834 on 1 and 910 DF, p-value: &lt; 2.2e-16 The slopes are the same in both models, but in the mean-centered model the y-intercept is now the average TotalKg. "],
["multiple-regression.html", "8 Multiple Regression Importing Viewing Formatting Modeling", " 8 Multiple Regression In this chapter we’ll use the lm() function for fitting a multiple regression model with two continuous predictors, which will take the following forms: \\[y_i=\\beta_0+\\beta_1\\left(X1\\right)+\\beta_2\\left(X2\\right)+\\epsilon_i\\] \\[y_i=\\beta_0+\\beta_1\\left(X1\\right)+\\beta_2\\left(X2\\right)+\\beta_3\\left(X1\\ \\cdot\\ X2\\right)+\\epsilon_i\\] \\[y_i\\ =\\ \\beta_0+\\beta_1\\left(X1\\right)+\\beta_2\\left(X1^2\\right)+\\epsilon_i\\] The first equation is the unmoderated (additive) model, the second is the moderated (multiplicative) model, and the third is the nonlinear model. The code in this chapter only works if you’re following along with the Github folder for this book (which you can download here), you’ve correctly set your working directory to the data folder (which you can learn how to do in Chapter 4), and run the code in the order it appears in this chapter. Importing We’ll be importing two datasets for this chapter: age_height_weight.txt and data_altitude.csv. The age_height_weight.txt file will be used for the unmoderated model, and the data_altitude.csv file will be used for the moderated and nonlinear models. data1 &lt;- read.delim(&quot;age_height_weight.txt&quot;, sep = &quot;&quot;) data2 &lt;- read.csv(&quot;data_altitude.csv&quot;) Viewing Data for Unmoderated Model As the name implies, the age_height_weight.txt file contains age, height, and weight data for several males and females. You can use the str() and head() functions to get the overall “impression” of the dataset. str(data1) &#39;data.frame&#39;: 19 obs. of 5 variables: $ Name : chr &quot;Alfred&quot; &quot;Alice&quot; &quot;Barbara&quot; &quot;Carol&quot; ... $ Sex : chr &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; ... $ Age : int 14 13 13 14 14 12 12 15 13 12 ... $ Height: num 69 56.5 65.3 62.8 63.5 57.3 59.8 62.5 62.5 59 ... $ Weight: num 112 84 98 102 102 ... The str() function provides a lot of good information. We now know that the dataset was correctly imported as a data frame which consists of 19 observations of 5 variables, and we know the column names and column variable types. Additionally, the first few observations for each column is printed. But if you’d rather look at the dataset in a more standard format, you can use the head() function to see the first few observations: head(data1) Name Sex Age Height Weight 1 Alfred M 14 69.0 112.5 2 Alice F 13 56.5 84.0 3 Barbara F 13 65.3 98.0 4 Carol F 14 62.8 102.5 5 Henry M 14 63.5 102.5 6 James M 12 57.3 83.0 We can use the plot() function to visualize the relationships between Age, Height, and Weight: # Used brackets to subset the data and select columns 3, 4, 5 plot(data1[c(3,4,5)]) Or, you could visualize all three variables on one graph with a 3D scatterplot: scatterplot3d(x = data1$Height, z = data1$Weight, y = data1$Age, main = &quot;Weight, Age, and Height&quot;, pch = 16, highlight.3d = TRUE, type = &quot;h&quot;, grid = TRUE, box = FALSE, xlab = &quot;Height (in)&quot;, ylab = &quot;Age (yrs)&quot;, zlab = &quot;Weight (lbs)&quot;, angle = 55) We can use the hist() function to visualize the distributions of Age, Height, and Weight: par(mfrow = c(1,3)) hist(data1$Age) hist(data1$Height) hist(data1$Weight) The code par(mfrow = c(2,2)) above is used to print the plots as a 2 x 2 grid; the plot() function does not need to be used in conjuction with this piece of code. And you can look at the six number summary with the summary() function: summary(data1[c(3,4,5)]) Age Height Weight Min. :11.00 Min. :51.30 Min. : 50.50 1st Qu.:12.00 1st Qu.:58.25 1st Qu.: 84.25 Median :13.00 Median :62.80 Median : 99.50 Mean :13.32 Mean :62.34 Mean :100.03 3rd Qu.:14.50 3rd Qu.:65.90 3rd Qu.:112.25 Max. :16.00 Max. :72.00 Max. :150.00 Data for Moderated and Nonlinear Models The data_altitude.csv file contains data about an athlete’s baseline VO2max, BVO2, and a VO2max value where the test was conducted at some altitude, AVO2. We’ll use the str() and head() functions to view the dataset: str(data2) &#39;data.frame&#39;: 105 obs. of 5 variables: $ subID : int 1 2 3 4 5 6 7 8 9 10 ... $ altitude: int 2300 2300 2500 3200 4340 2660 4300 5400 5400 5400 ... $ BVO2 : num 74 72.4 44.6 57.3 45.1 ... $ AVO2 : num 61.4 60 42.1 52.5 36.9 ... $ decrease: num -12.6 -12.4 -2.5 -4.8 -8.2 ... head(data2) subID altitude BVO2 AVO2 decrease 1 1 2300 74.00 61.42 -12.58 2 2 2300 72.40 60.00 -12.40 3 3 2500 44.60 42.10 -2.50 4 4 3200 57.30 52.50 -4.80 5 5 4340 45.10 36.90 -8.20 6 6 2660 44.13 38.45 -5.68 More examples of viewing data can be found in Chapter 5 Formatting Data for Moderated and Nonlinear Models The only formatting that needs to be done is to change the unit of altitude from meters to kilometers, so that the axes have similar scales. Also, we’ll create a column that squares all of the alt_km values which will be used for the nonlinear model. To accomplish this, we can use the mutate() function which comes from the tidyverse package. Make sure the tidyverse package is loaded before running this code: data2 &lt;- mutate(data2, alt_km = altitude / 1000) # Divide altitude values by 1000, and save values to new column data2 &lt;- mutate(data2, alt_km_sq = alt_km^2) # Square alt_km values, and save values to a new column head(data2) subID altitude BVO2 AVO2 decrease alt_km alt_km_sq 1 1 2300 74.00 61.42 -12.58 2.30 5.2900 2 2 2300 72.40 60.00 -12.40 2.30 5.2900 3 3 2500 44.60 42.10 -2.50 2.50 6.2500 4 4 3200 57.30 52.50 -4.80 3.20 10.2400 5 5 4340 45.10 36.90 -8.20 4.34 18.8356 6 6 2660 44.13 38.45 -5.68 2.66 7.0756 More examples of formatting data can be found in Chapter 6 Modeling The lm() Function lm(formula, data, subset, weights, na.action, method = &quot;qr&quot;, model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, offset, ...) The lm() (linear model) function is used for fitting linear models. There are many arguments for this function, but the formula and dataarguments are the only ones that need to be specified. If you’d like to learn more about functions and arguments, Chapter 2 covers basic programming concepts, including functions and arguments. The lm() function automatically controls for shared variance, so we don’t need to take any additional steps before adding the variables to the model. We’ll use the lm() function to make three models: an unmoderated model, a moderated model, and a nonlinear model. Unmoderated Model In the lm() function, the dependent variable is listed first, followed by a tilde, ~, and then the independent variable(s). The data argument is set equal to the object that contains the dataset, which in this example is the object called data1. In the unmoderated model, the addition operator, +, is used between each independent variable. lm(formula = Weight ~ Age + Height, data = data1) Call: lm(formula = Weight ~ Age + Height, data = data1) Coefficients: (Intercept) Age Height -141.224 1.278 3.597 The function prints out the slope and intercept for the model. To see more information about the model, the model needs to be saved as an object: my_model &lt;- lm(formula = Weight ~ Age + Height, data = data1) Now that we’ve saved the model as an object, called my_model, we can access a lot of information about the model. Three good functions to remember are summary(), plot(), and attributes(). summary(my_model) Call: lm(formula = Weight ~ Age + Height, data = data1) Residuals: Min 1Q Median 3Q Max -17.962 -6.010 -0.067 7.553 20.796 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -141.2238 33.3831 -4.230 0.000637 *** Age 1.2784 3.1101 0.411 0.686492 Height 3.5970 0.9055 3.973 0.001093 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 11.51 on 16 degrees of freedom Multiple R-squared: 0.7729, Adjusted R-squared: 0.7445 F-statistic: 27.23 on 2 and 16 DF, p-value: 7.074e-06 Now we have a nice summary print-out which includes the F-statistic, Residuals, R-squared, p-value, and more. The plot function can be used on the my_model object to view Residuals vs Fitted, Normal Q-Q, Scale-Location and Residuals vs Leverage plots. par(mfrow = c(2,2)) plot(my_model) The code par(mfrow = c(2,2)) above is used to print the plots as a 2 x 2 grid; the plot() function does not need to be used in conjuction with this piece of code. The my_model object has attributes. Attributes can store additional information about the object, such as the model’s residuals. You can find out the attributes of an object with the attributes() function. attributes(my_model) $names [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; $class [1] &quot;lm&quot; The model’s attributes can be accessed by using a dollar sign, $. For example, here’s a printout of the first 5 fitted values of the model: my_model$fitted.values[1:5] 1 2 3 4 5 124.86856 78.62734 110.28117 102.56700 105.08492 Moderated Model In the lm() function, the dependent variable is listed first, followed by a tilda, ~, and then the independent variable(s). The data argument is set equal to the object that contains the dataset, which in this example is the object called data2. In the moderated model, the multiplication operator, *, is used between each independent variable to indicate an interaction. my_model2 &lt;- lm(formula = decrease ~ alt_km * BVO2, data = data2) summary(my_model2) Call: lm(formula = decrease ~ alt_km * BVO2, data = data2) Residuals: Min 1Q Median 3Q Max -7.2307 -1.4511 -0.1014 1.2200 6.9343 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 8.51588 3.01436 2.825 0.005697 ** alt_km -0.11323 1.00029 -0.113 0.910097 BVO2 -0.10874 0.05266 -2.065 0.041512 * alt_km:BVO2 -0.07448 0.01849 -4.027 0.000109 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.432 on 101 degrees of freedom Multiple R-squared: 0.8488, Adjusted R-squared: 0.8443 F-statistic: 189 on 3 and 101 DF, p-value: &lt; 2.2e-16 Nonlinear Model In the lm() function, the dependent variable is listed first, followed by a tilda, ~, and then the independent variable(s). The data argument is set equal to the object that contains the dataset, which in this example is the object called data2. In the nonlinear model, the addition operator, +, is used between each independent variable. In this example, the independent variables are the alt_km and alt_km_sq columns. my_model3 &lt;- lm(formula = decrease ~ alt_km + alt_km_sq, data = data2) summary(my_model3) Call: lm(formula = decrease ~ alt_km + alt_km_sq, data = data2) Residuals: Min 1Q Median 3Q Max -9.3091 -2.4191 0.9296 2.4161 7.0271 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -2.2708 1.5327 -1.482 0.1415 alt_km -0.6921 1.0994 -0.630 0.5304 alt_km_sq -0.4532 0.1766 -2.567 0.0117 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 3.687 on 102 degrees of freedom Multiple R-squared: 0.6491, Adjusted R-squared: 0.6422 F-statistic: 94.35 on 2 and 102 DF, p-value: &lt; 2.2e-16 "],
["one-and-two-sample-t-test.html", "9 One and Two Sample t-test Importing Viewing Formatting Modeling", " 9 One and Two Sample t-test In this chapter we’ll use the t.test() function for fitting one and two sample t-test models, which will take the following forms: \\[y_i=\\beta_0+\\beta_1\\left(X1\\right)+\\epsilon_i\\] \\[y_i=\\beta_0+\\beta_1\\left(X1\\right)+U_i+\\epsilon_{ij}\\] The first equation is the two sample independent t-test model, and the second is the two sample dependent t-test model. The code in this chapter only works if you’re following along with the Github folder for this book (which you can download here), you’ve correctly set your working directory to the data folder (which you can learn how to do in Chapter 4), and run the code in the order it appears in this chapter. Importing For this chapter we’ll be using three datasets: rmr.csv for the two sample indepedent t-test, lactate_threshold.csv for the two sample dependent t-test, and the third dataset for the one sample t-test will be created in R. First we’ll create the data for the one sample t-test. The code below creates a random normal distribution of 100 samples with a mean of 54.3 and a standard deviation of 5.3, which are both arbitrary values. set.seed(1) data1 &lt;- rnorm(n = 100, mean = 54.3, sd = 5.3) The set.seed() function is used to ensure that the same random normal distribution is created every time. Random normal distributions created in R are not truly random, and the values can be replicated with the set.seed() function. Doing this will allow you to copy and past the code into your R session and get the same results. And we’ll import the two other datasets: data2 &lt;- read.csv(&quot;rmr.csv&quot;) data3 &lt;- read.csv(&quot;lactate_threshold.csv&quot;) Viewing Data for Two Sample Independent The dataset for the two sample independent t-test contains resting metabolic rate values, RMR, for several males and females, which was collected using a metabolic cart. head(data2) SubID Sex VO2_abs VO2_rel RER kcal_L RMR 1 1 M 0.28 4.1 0.80 4.80 1935.360 2 2 M 0.29 3.6 0.92 4.92 2054.592 3 3 M 0.29 2.8 0.94 4.99 2083.824 4 4 M 0.52 7.3 0.89 4.92 3684.096 5 5 M 0.28 3.4 0.86 4.86 1959.552 6 6 M 0.27 3.7 0.91 4.92 1912.896 str(data2) &#39;data.frame&#39;: 16 obs. of 7 variables: $ SubID : int 1 2 3 4 5 6 7 8 9 10 ... $ Sex : chr &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ... $ VO2_abs: num 0.28 0.29 0.29 0.52 0.28 0.27 0.32 0.3 0.23 0.17 ... $ VO2_rel: num 4.1 3.6 2.8 7.3 3.4 3.7 3.4 3.2 5.2 2.8 ... $ RER : num 0.8 0.92 0.94 0.89 0.86 0.91 1.07 0.89 0.86 0.83 ... $ kcal_L : num 4.8 4.92 4.99 4.92 4.86 4.92 5.05 4.92 4.86 4.83 ... $ RMR : num 1935 2055 2084 3684 1960 ... Data for Two Sample Dependent The dataset for the two sample dependent t-test contains heart rate and VO2 data at lactate threshold and ventilatory threshold for several males and females. Lactate threshold is the point at which the blood concentration of lactate begins to increase exponentially ( source). Ventilatory threshold is the point during exercise at which ventilation starts to increase at a faster rate than VO2 (source). head(data3) SubID VT_VO2_abs VT_HR LT_VO2_abs LT_HR 1 1 3.3 179 3.3 188 2 2 2.7 185 2.6 177 3 3 4.3 182 4.4 175 4 4 3.3 188 3.5 186 5 5 2.5 191 2.4 183 6 6 2.6 193 2.5 193 str(data3) &#39;data.frame&#39;: 24 obs. of 5 variables: $ SubID : int 1 2 3 4 5 6 7 8 9 10 ... $ VT_VO2_abs: num 3.3 2.7 4.3 3.3 2.5 2.6 3.77 1.58 4.22 2.51 ... $ VT_HR : num 179 185 182 188 191 193 183 187 197 188 ... $ LT_VO2_abs: num 3.3 2.6 4.4 3.5 2.4 2.5 3.7 1.5 3.8 2.3 ... $ LT_HR : num 188 177 175 186 183 193 179 186 193 188 ... More examples of viewing data can be found in Chapter 5 Formatting Data for Two Sample Independent We’ll be comparing the resting metabolic rate of males and females for the independent t-test, so we need to filter the data to include only males or females, but not both. There are many ways to do this in R, and one way is to split the data2 object into data_M and data_F objects, which only contains data for the males and females, respectively. This can be accomplished with the filter() function. The filter() function comes from the tidyverse package, so make sure you’ve loaded that library into your workspace. data2_M &lt;- filter(data2, Sex == &quot;M&quot;) data2_F &lt;- filter(data2, Sex == &quot;F&quot;) # Make sure you&#39;ve loaded the tidyverse package Now the dataset has been separated into two separate objects which each contain only one sex. head(data2_M) SubID Sex VO2_abs VO2_rel RER kcal_L RMR 1 1 M 0.28 4.1 0.80 4.80 1935.360 2 2 M 0.29 3.6 0.92 4.92 2054.592 3 3 M 0.29 2.8 0.94 4.99 2083.824 4 4 M 0.52 7.3 0.89 4.92 3684.096 5 5 M 0.28 3.4 0.86 4.86 1959.552 6 6 M 0.27 3.7 0.91 4.92 1912.896 head(data2_F) SubID Sex VO2_abs VO2_rel RER kcal_L RMR 1 9 F 0.23 5.2 0.86 4.86 1609.632 2 10 F 0.17 2.8 0.83 4.83 1182.384 3 11 F 0.25 5.2 0.92 5.20 1872.000 4 12 F 0.29 3.5 0.83 4.83 2017.008 5 13 F 0.29 4.4 0.79 4.78 1996.128 6 14 F 0.27 3.9 0.90 4.92 1912.896 More examples of formatting data can be found in Chapter 6 Modeling The t.test()Function t.test(x, y = NULL, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, ...) The t.test() function can be used for both one sample and two sample tests. For two sample tests, the observations can be either dependent or independent. Notice that there are many arguments for this function, but the x argument is the only argument that needs to be specified for one sample tests. x and y are the two arguments that need to be specified for two sample independent t-tests, and x, y, and paired need to be specified for dependent t-tests. If you’d like to learn more about functions and arguments, Chapter 2 covers basic programming concepts, including functions and arguments. One Sample For the one sample t-test, the x argument should be set equal to the object that contains the dataset, which in this case is the object data1. If the data object contained multiple columns then you would need to specify the column to use in your analysis (for example: data1$Column1), but in this example the data1 object only has the one column. The default value for the population mean, mu, is 0, but in this made up example we’ll say mu is equal to 49.1. t.test(x = data1, mu = 49.1) One Sample t-test data: data1 t = 12.136, df = 99, p-value &lt; 2.2e-16 alternative hypothesis: true mean is not equal to 49.1 95 percent confidence interval: 53.93253 55.82168 sample estimates: mean of x 54.8771 Two Sample: Independent / Unpaired The goal of this analysis is to determine if there is a difference between the resting metabolic rate of males and females, on average. The x and y arguments should be set equal to the object(s) that contain the data you want to compare. In this example, those are the data_M and data_F objects. Specifically, we want to compare the RMR column from each dataset, which can be selected with the dollar sign, $. By default, paired is set equal to FALSE, but it’s written out explicitly below to make the code more clear. t.test(x = data2_M$RMR, y = data2_F$RMR, paired = FALSE) Welch Two Sample t-test data: data2_M$RMR and data2_F$RMR t = 2.2322, df = 10.025, p-value = 0.04959 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 1.119132 1031.252868 sample estimates: mean of x mean of y 2260.350 1744.164 Two Sample: Dependent / Paired The goal of this analysis is to determine if lactate and ventilatory thresholds occur at the same exercise intensity, where exercise intensity is measured as VO2 consumption. The x and y arguments should be set equal to the object(s) that contain the data you want to compare, which is this example is the data3 object. Specifically, we want to compare the VT_VO2_abs and LT_VO2_abs columns from the dataset, which can be selected with the dollar sign, $. By default, paired is set equal to FALSE, which needs to be changed to TRUE. t.test(x = data3$VT_VO2_abs, y = data3$LT_VO2_abs, paired = TRUE) Paired t-test data: data3$VT_VO2_abs and data3$LT_VO2_abs t = 1.0898, df = 23, p-value = 0.2871 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -0.05052285 0.16302285 sample estimates: mean of the differences 0.05625 Optional Arguments There are additional arguments for the t-test() function that can be specified. By default, alternative is set equal to \"two.sided\", but this can be changed to \"less\" or \"greater\". var.equal is set equal to FALSE by default, but this can be set to TRUE. 0.95 is the default confidence level (conf.level), but this can be set to any desired confidence level. "],
["one-way-anova.html", "10 One-Way ANOVA Importing Viewing Formatting Modeling", " 10 One-Way ANOVA In this chapter we’ll use the lm() and ezANOVA() functions for fitting a one-way ANOVA model with dummy codes and contrast codes, which will take the following form: \\[y_i=\\beta_0+\\beta_1\\left(X1_L\\right)+\\beta_2\\left(X1_Q\\right)+\\epsilon_i\\] The code in this chapter only works if you’re following along with the Github folder for this book (which you can download here), you’ve correctly set your working directory to the data folder (which you can learn how to do in Chapter 4), and run the code in the order it appears in this chapter. Importing For this chapter we’ll be using the data_800m.csv file. This dataset contains information about a subject’s VO2max and their 800 meter run time. data &lt;- read.csv(&quot;data_800m.csv&quot;) Viewing The data_800m.csv dataset contains information about a subject’s VO2max and their 800 meter run time. Base off of that run time, they were categorized as faster, medium, or slower. head(data) subID VO2 time groups 1 s001 74.00 116 faster 2 s002 68.00 117 faster 3 s003 70.50 119 faster 4 s004 61.00 121 faster 5 s005 73.25 121 faster 6 s006 68.25 122 faster More examples of viewing data can be found in Chapter 5 Formatting There’s no formatting that needs to be done for this dataset. By default, R will create dummy codes for categorical variables. Lambda 1 and Lambda 2 will be assigned the codes 0,1,0 and 0,0,1. However, we could create a set of contrast codes to analyze linear and quadratic effects. We’ll recode the faster, medium, and slower categories as -1, 0, 1 and 0, 1, 2 for the linear and quadratic codes, respectively. One easy way to do this is with the mapvalues() function which comes from the plyr package. You’ll first need to install the package (install.packages(plyr)) and then load the library (library(plyr)) before using this function. data$lin.c &lt;- mapvalues(x = data$groups, from = c(&quot;faster&quot;, &quot;medium&quot;, &quot;slower&quot;), to = c(1,0,-1)) data$quad.c &lt;- mapvalues(x = data$groups, from = c(&quot;faster&quot;, &quot;medium&quot;, &quot;slower&quot;), to = c(-1,2,-1)) More examples of formatting data can be found in Chapter 6 Modeling The ezANOVA()Function ezANOVA( data , dv , wid , within = NULL , within_full = NULL , within_covariates = NULL , between = NULL , between_covariates = NULL , observed = NULL , diff = NULL , reverse_diff = FALSE , type = 2 , white.adjust = FALSE , detailed = FALSE , return_aov = FALSE ) The ezANOVA() function is used for ANOVA models. There are many arguments for this function, but not all of them need to be specified. The ones that do need to be specified will depend on the exact ANOVA that you’re performing. If you’d like to learn more about functions and arguments, Chapter 2 covers basic programming concepts, including functions and arguments. The lm() Function lm(formula, data, subset, weights, na.action, method = &quot;qr&quot;, model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, offset, ...) The lm() (linear model) function is used for fitting linear models. Notice that there are many arguments for this function, but the formula and dataarguments are the only ones that need to be specified. If you’d like to learn more about functions and arguments, Chapter 2 covers basic programming concepts, including functions and arguments. Model with Dummy Codes By default, the lm() and ezANOVA() functions use the dummy codes 0, 1, 0 and 0, 0, 1 for three categories. In the formatting section we created contrast codes, but in this example we will not use those contrast codes. We’ll let the function create dummy codes for us. In this example we’ll use the ezANOVA() function. We need to specify the data, dv, wid, between, and type arguments. You can also set detailed and return_aov to TRUE for a more comprehensive output. The data argument is set equal to the entire dataset, the dv argument, which is short for dependent variable, is set equal to the column of the dependent variable. The wid argument is set equal to the column that contains the variable specifying the case/Ss identifier; usually the subID column, or equivalent. The between argument is set equal to the between-subjects factor(s), which in this case is the groups column. ez::ezANOVA(data = data, dv = VO2, wid = subID, between = .(groups), type = 3, detailed = TRUE, return_aov = TRUE) $ANOVA Effect DFn DFd SSn SSd F p p&lt;.05 1 (Intercept) 1 18 82657.440 398.9286 3729.574754 2.528806e-22 * 2 groups 2 18 398.756 398.9286 8.996106 1.956932e-03 * ges 1 0.9951969 2 0.4998918 $`Levene&#39;s Test for Homogeneity of Variance` DFn DFd SSn SSd F p p&lt;.05 1 2 18 18.05357 168.1071 0.9665392 0.3992852 $aov Call: aov(formula = formula(aov_formula), data = data) Terms: groups Residuals Sum of Squares 398.7560 398.9286 Deg. of Freedom 2 18 Residual standard error: 4.707728 Estimated effects may be unbalanced Notice the formatting in the code above. When using the between and within arguments, you need to use a period and parentheses, .(), when listing the column. You can list additional columns for both of these arguments by separating each column with a comma: between = .(groups, Column2). Model with Contrast Codes Rather than using the default dummy codes, we can use the set of contrast codes that were created in the formatting section above. We’ll use the lm() function to implement this model. my_model &lt;- lm(formula = VO2 ~ lin.c + quad.c, data = data) summary(my_model) Call: lm(formula = VO2 ~ lin.c + quad.c, data = data) Residuals: Min 1Q Median 3Q Max -7.964 -1.750 -0.500 2.250 8.286 Coefficients: (1 not defined because of singularities) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 57.964 1.779 32.576 &lt; 2e-16 *** lin.c0 3.786 2.516 1.504 0.149817 lin.c1 10.536 2.516 4.187 0.000554 *** quad.c2 NA NA NA NA --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.708 on 18 degrees of freedom Multiple R-squared: 0.4999, Adjusted R-squared: 0.4443 F-statistic: 8.996 on 2 and 18 DF, p-value: 0.001957 Anova(my_model, type = &quot;III&quot;, singular.ok = T) Anova Table (Type III tests) Response: VO2 Sum Sq Df F values Pr(&gt;F) lin.c 388.50 1 17.53 0.0005542 *** quad.c 0.00 0 Residuals 398.93 18 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For comparison purposes, here’s what the model printout looks like with the dummy codes using the lm() function: summary(lm(VO2 ~ groups, data = data)) Call: lm(formula = VO2 ~ groups, data = data) Residuals: Min 1Q Median 3Q Max -7.964 -1.750 -0.500 2.250 8.286 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 68.500 1.779 38.497 &lt; 2e-16 *** groupsmedium -6.750 2.516 -2.682 0.015209 * groupsslower -10.536 2.516 -4.187 0.000554 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.708 on 18 degrees of freedom Multiple R-squared: 0.4999, Adjusted R-squared: 0.4443 F-statistic: 8.996 on 2 and 18 DF, p-value: 0.001957 "],
["repeated-measures-anova.html", "11 Repeated Measures ANOVA Importing Viewing Formatting Modeling", " 11 Repeated Measures ANOVA In this example we’ll use the ezANOVA() function for fitting a Repeated-Measures ANOVA model, which will take the following form: \\[y_i=\\beta_0+\\beta_1\\left(X1_L\\right)+\\beta_2\\left(X1_Q\\right)+U_i+\\epsilon_{ij}\\] The code in this chapter only works if you’re following along with the Github folder for this book (which you can download here), you’ve correctly set your working directory to the data folder (which you can learn how to do in Chapter 4), and run the code in the order it appears in this chapter. Importing We’ll be using the cardiovascular.csv file for this example. data &lt;- read.csv(&quot;cardiovascular.csv&quot;) Viewing This dataset contains mean arterial pressure values of subjects during Rest and at four time points while exercising, MAP_T1:T4. The time points were spaced equally apart and correspond to a linearly increasing exercise workload on a treadmill. head(data) SubID Rest MAP_T1 MAP_T2 MAP_T3 MAP_T4 1 1 94.33333 102.00000 112.6667 115.3333 115.6667 2 2 90.33333 88.33333 106.0000 106.6667 114.0000 3 3 103.00000 98.33333 119.0000 118.6667 121.3333 4 4 106.33333 123.00000 134.0000 130.6667 134.0000 5 5 97.66667 105.66667 101.0000 109.0000 104.6667 6 6 97.00000 100.66667 99.0000 102.6667 101.0000 More examples of viewing data can be found in Chapter 5 Formatting The data should be converted to long format to make it compatible with the ezANOVA() function. We can use the pivot_longer() function from the tidyverse package. Make sure you have that package loaded before using this function. data &lt;- pivot_longer(data = data, cols = c(&quot;Rest&quot;, &quot;MAP_T1&quot;, &quot;MAP_T2&quot;, &quot;MAP_T3&quot;, &quot;MAP_T4&quot;), names_to = &quot;Time&quot;, values_to = &quot;MAP&quot;) More examples of formatting data can be found in Chapter 6 Modeling The ezANOVA()Function ezANOVA( data , dv , wid , within = NULL , within_full = NULL , within_covariates = NULL , between = NULL , between_covariates = NULL , observed = NULL , diff = NULL , reverse_diff = FALSE , type = 2 , white.adjust = FALSE , detailed = FALSE , return_aov = FALSE ) The ezANOVA() function is used for ANOVA models. There are many arguments for this function, but not all of them need to be specified. The ones that do need to be specified will depend on the exact ANOVA that you’re performing. If you’d like to learn more about functions and arguments, Chapter 2 covers basic programming concepts, including functions and arguments. Model In this example we need to specify the data, dv, wid, and within arguments. The data argument is set equal to the entire dataset, the dv argument, which is short for dependent variable, is set equal to the column of the dependent variable. The wid argument is set equal to the column that contains the variable specifying the case/Ss identifier; usually the subID column, or equivalent. The within argument is set equal to the within-subject factor(s). ez::ezANOVA(data = data, dv = MAP, wid = SubID, within = .(Time)) $ANOVA Effect DFn DFd F p p&lt;.05 ges 2 Time 4 76 24.5791 4.476507e-13 * 0.2902555 $`Mauchly&#39;s Test for Sphericity` Effect W p p&lt;.05 2 Time 0.2282138 0.002359703 * $`Sphericity Corrections` Effect GGe p[GG] p[GG]&lt;.05 HFe p[HF] p[HF]&lt;.05 2 Time 0.6209806 6.49275e-09 * 0.7216975 5.046718e-10 * Notice the formatting in the code above. When using the within and between arguments, you need to use a period and parentheses, .(), when listing the column. You can list additional columns for both of these arguments by separating each column with a comma: within = .(Time, Column2). "],
["mixed-factorial-anova.html", "12 Mixed-Factorial ANOVA Importing Viewing Formatting Modeling", " 12 Mixed-Factorial ANOVA In this example we’ll use the ezANOVA() function for fitting a Mixed-Factorial ANOVA model with one crossed (within-subject) factor, and one nested (between-subjects) factor, which will take the following form: \\[y_i=\\beta_0+\\beta_1\\left(X1\\right)+\\beta_2\\left(X2\\right)+\\beta_3\\left(X1\\cdot X2\\right)+U_i+\\epsilon_{ij}\\] The code in this chapter only works if you’re following along with the Github folder for this book (which you can download here), you’ve correctly set your working directory to the data folder (which you can learn how to do in Chapter 4), and run the code in the order it appears in this chapter. Importing We’ll be using the flexbility_power.csv file for this example. data &lt;- read.csv(&quot;flexibility_power.csv&quot;) Viewing In this dataset, male and female subjects performed a countermovement vertical jump under three conditions: Baseline, Post_run, and Post_stretch, where the Post_run and Post_stretch conditions consisted of running and stretching (lower body) for 10 minutes, respectively, just prior to the vertical jump. Half of the students completed the Post_run first and vice versa. The subjects were instructed to try to jump as high as possible under each condition. Vertical jump peak power was then calculated as a function of jump height and body mass. head(data) ID Sex Baseline Post_run Post_stretch 1 1 M 2586.75 2638.65 2560.80 2 2 M 3246.45 3428.10 3454.05 3 3 M 5701.35 5701.35 5779.20 4 4 M 2365.20 2417.10 2417.10 5 5 M 3023.85 3257.40 3101.70 6 6 M 2055.30 2185.05 2107.20 The data reported under the Baseline, Post_run, and Post_stretch columns is vertical jump peak power, in watts. More examples of viewing data can be found in Chapter 5 Formatting The data should be converted to long format to make it compatible with the ezANOVA() function. We can use the pivot_longer() function from the tidyverse package. Make sure you have that package loaded before using this function. data &lt;- pivot_longer(data = data, cols = c(&quot;Baseline&quot;, &quot;Post_run&quot;, &quot;Post_stretch&quot;), names_to = &quot;Condition&quot;, values_to = &quot;Power_Watts&quot;) More examples of formatting data can be found in Chapter 6 Modeling The ezANOVA()Function ezANOVA( data , dv , wid , within = NULL , within_full = NULL , within_covariates = NULL , between = NULL , between_covariates = NULL , observed = NULL , diff = NULL , reverse_diff = FALSE , type = 2 , white.adjust = FALSE , detailed = FALSE , return_aov = FALSE ) The ezANOVA() function is used for ANOVA models. There are many arguments for this function, but not all of them need to be specified. The ones that do need to be specified will depend on the exact ANOVA that you’re performing. If you’d like to learn more about functions and arguments, Chapter 2 covers basic programming concepts, including functions and arguments. Model In this example we need to specify the data, dv, wid, within and between arguments. The data argument is set equal to the entire dataset, the dv argument, which is short for dependent variable, is set equal to the column of the dependent variable. The wid argument is set equal to the column that contains the variable specifying the case/Ss identifier; usually the subID column, or equivalent. The within and between arguments are set equal to the within-subject and between-subjects factor(s). ezANOVA(data = data, dv = Power_Watts, wid = ID, within = .(Condition), between = .(Sex)) $ANOVA Effect DFn DFd F p p&lt;.05 ges 2 Sex 1 19 3.627466 7.208602e-02 0.1596944133 3 Condition 2 38 11.885433 9.794858e-05 * 0.0028620916 4 Sex:Condition 2 38 1.091126 3.461285e-01 0.0002634351 $`Mauchly&#39;s Test for Sphericity` Effect W p p&lt;.05 3 Condition 0.763975 0.08865681 4 Sex:Condition 0.763975 0.08865681 $`Sphericity Corrections` Effect GGe p[GG] p[GG]&lt;.05 HFe p[HF] 3 Condition 0.8090451 0.0003445542 * 0.8733736 0.0002253116 4 Sex:Condition 0.8090451 0.3365253953 0.8733736 0.3401654081 p[HF]&lt;.05 3 * 4 Notice the formatting in the code above. When using the between and within arguments, you need to use a period and parentheses, .() when listing the column. You can list additional columns for both of these arguments by separating each column with a comma: within = .(Condition, Column2). "],
["analysis-of-covariance-ancova.html", "13 Analysis of Covariance (ANCOVA) Importing Viewing Modeling", " 13 Analysis of Covariance (ANCOVA) In this example we’ll use the aov() function for fitting one and two sample t-test models with one continuous predictor, which will take the following form: \\[y_i=\\beta_0+\\beta_1\\left(X1\\right)+\\beta_2\\left(X2\\right)+\\epsilon_i\\] The code in this chapter only works if you’re following along with the Github folder for this book (which you can download here), you’ve correctly set your working directory to the data folder (which you can learn how to do in Chapter 4, and run the code in the order it appears in this chapter. Importing data &lt;- read.csv(&quot;power.csv&quot;) Viewing In this dataset, males and females performed a maximal effort Wingate Test, which is a 30 second test usually performed on a stationary bike that’s used to assess anaerobic leg power. The participant’s maximum achieved power, WG_power_watts, mass, mass_kg, and several other variables were recorded. head(data) subject sex self_ID mass_kg VJ_power_watts WG_power_watts 1 1 M endurance 65.00000 4203.498 809 2 2 M power 80.00000 5332.476 995 3 3 M power 71.00000 4430.985 856 4 4 M endurance 64.50000 3256.266 595 5 5 M endurance 73.00000 4001.481 707 6 6 M power 81.81818 5289.559 956 ggplot(data, aes(x = mass_kg, y = WG_power_watts, color = sex)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_bw() + scale_color_brewer(palette = &quot;Dark2&quot;) + labs(title = &quot;Wingate Power vs Mass in Males and Females&quot;) + xlab(&quot;Mass (kg)&quot;) + ylab(&quot;Wingate Power (Watts)&quot;) More examples of viewing data can be found in Chapter 5 Modeling The aov()Function aov(formula, data = NULL, projections = FALSE, qr = TRUE, contrasts = NULL, ...) The aov function has several arguments, but the only ones that need to be specified are the formula and data arguments. Model When using the aov() function, the formula argument is set equal to the dependent variable, followed by a tilde, ~, and then the independent variable(s). The data argument is set equal to the object that contains the dataset, which in this example is the object called data. my_model &lt;- aov(formula = WG_power_watts ~ sex + mass_kg, data = data) You can use the summary.lm() function to see a summary of the model. summary.lm(my_model) Call: aov(formula = WG_power_watts ~ sex + mass_kg, data = data) Residuals: Min 1Q Median 3Q Max -168.68 -79.85 25.63 58.61 230.43 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -35.170 133.175 -0.264 0.7943 sexM 106.975 55.674 1.921 0.0683 . mass_kg 10.727 2.096 5.117 4.55e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 102.9 on 21 degrees of freedom Multiple R-squared: 0.7558, Adjusted R-squared: 0.7326 F-statistic: 32.5 on 2 and 21 DF, p-value: 3.724e-07 "]
]
